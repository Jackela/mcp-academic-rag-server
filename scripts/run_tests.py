#!/usr/bin/env python3
"""
å‘é‡å­˜å‚¨æµ‹è¯•è¿è¡Œè„šæœ¬

æä¾›ä¸åŒç±»å‹æµ‹è¯•çš„ä¾¿æ·è¿è¡Œæ–¹å¼ï¼Œæ”¯æŒæµ‹è¯•ç­›é€‰ã€æŠ¥å‘Šç”Ÿæˆå’Œç»“æœåˆ†æã€‚
"""

import argparse
import os
import sys
import subprocess
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any


class TestRunner:
    """æµ‹è¯•è¿è¡Œå™¨ç±»"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.test_results_dir = self.project_root / "test-results"
        self.coverage_dir = self.project_root / "htmlcov"
        
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        self.test_results_dir.mkdir(exist_ok=True)
        
    def run_command(self, command: List[str], description: str = None) -> bool:
        """è¿è¡Œå‘½ä»¤å¹¶è¿”å›æ˜¯å¦æˆåŠŸ"""
        if description:
            print(f"\nğŸ”§ {description}")
        
        print(f"Running: {' '.join(command)}")
        
        try:
            result = subprocess.run(
                command,
                cwd=self.project_root,
                capture_output=False,
                text=True
            )
            
            success = result.returncode == 0
            if success:
                print(f"âœ… {description or 'Command'} completed successfully")
            else:
                print(f"âŒ {description or 'Command'} failed with code {result.returncode}")
            
            return success
            
        except Exception as e:
            print(f"âŒ Error running {description or 'command'}: {e}")
            return False
    
    def check_dependencies(self) -> Dict[str, bool]:
        """æ£€æŸ¥æµ‹è¯•ä¾èµ–æ˜¯å¦å¯ç”¨"""
        dependencies = {}
        
        # æ£€æŸ¥åŸºç¡€ä¾èµ–
        try:
            import pytest
            dependencies["pytest"] = True
        except ImportError:
            dependencies["pytest"] = False
        
        # æ£€æŸ¥FAISS
        try:
            import faiss
            dependencies["faiss"] = True
        except ImportError:
            dependencies["faiss"] = False
        
        # æ£€æŸ¥Milvuså®¢æˆ·ç«¯
        try:
            import pymilvus
            dependencies["pymilvus"] = True
        except ImportError:
            dependencies["pymilvus"] = False
        
        # æ£€æŸ¥è¦†ç›–ç‡å·¥å…·
        try:
            import coverage
            dependencies["coverage"] = True
        except ImportError:
            dependencies["coverage"] = False
        
        return dependencies
    
    def print_dependency_status(self):
        """æ‰“å°ä¾èµ–çŠ¶æ€"""
        print("\nğŸ“‹ Dependency Status:")
        dependencies = self.check_dependencies()
        
        for dep, available in dependencies.items():
            status = "âœ…" if available else "âŒ"
            print(f"  {status} {dep}")
        
        if not dependencies.get("pytest", False):
            print("\nâŒ PyTest is required. Install with: pip install pytest")
            return False
        
        return True
    
    def run_unit_tests(self, verbose: bool = True, coverage: bool = True) -> bool:
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        command = [
            sys.executable, "-m", "pytest",
            "tests/unit/",
            "-v" if verbose else "-q",
            "--tb=short",
            "--durations=10",
            f"--junitxml={self.test_results_dir / 'junit-unit.xml'}",
            "-m", "not slow"
        ]
        
        if coverage:
            command.extend([
                "--cov=document_stores",
                "--cov=utils.vector_migration",
                "--cov-report=html",
                "--cov-report=term-missing",
                f"--cov-report=xml:{self.test_results_dir / 'coverage-unit.xml'}"
            ])
        
        return self.run_command(command, "Running unit tests")
    
    def run_integration_tests(self, verbose: bool = True, coverage: bool = True) -> bool:
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        command = [
            sys.executable, "-m", "pytest",
            "tests/integration/",
            "-v" if verbose else "-q",
            "--tb=short",
            "--durations=10",
            f"--junitxml={self.test_results_dir / 'junit-integration.xml'}",
            "-m", "not slow and not requires_milvus"
        ]
        
        if coverage:
            command.extend([
                "--cov=document_stores",
                "--cov=utils.vector_migration",
                "--cov-report=html",
                "--cov-report=term-missing",
                f"--cov-report=xml:{self.test_results_dir / 'coverage-integration.xml'}"
            ])
        
        return self.run_command(command, "Running integration tests")
    
    def run_performance_tests(self, verbose: bool = True) -> bool:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        command = [
            sys.executable, "-m", "pytest",
            "tests/performance/",
            "-v" if verbose else "-q",
            "--tb=short",
            "--durations=20",
            f"--junitxml={self.test_results_dir / 'junit-performance.xml'}",
            "-m", "benchmark",
            "--timeout=300"
        ]
        
        return self.run_command(command, "Running performance tests")
    
    def run_all_tests(self, verbose: bool = True, coverage: bool = True) -> bool:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\nğŸš€ Running all vector storage tests...\n")
        
        success = True
        
        # å•å…ƒæµ‹è¯•
        if not self.run_unit_tests(verbose, coverage):
            success = False
        
        # é›†æˆæµ‹è¯•
        if not self.run_integration_tests(verbose, coverage):
            success = False
        
        return success
    
    def run_quick_tests(self) -> bool:
        """è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆè·³è¿‡æ…¢é€Ÿæµ‹è¯•ï¼‰"""
        command = [
            sys.executable, "-m", "pytest",
            "tests/unit/",
            "tests/integration/",
            "-q",
            "--tb=line",
            "-m", "not slow and not benchmark and not requires_milvus",
            "--maxfail=5"
        ]
        
        return self.run_command(command, "Running quick tests")
    
    def run_specific_tests(self, pattern: str, verbose: bool = True) -> bool:
        """è¿è¡Œç‰¹å®šæµ‹è¯•"""
        command = [
            sys.executable, "-m", "pytest",
            "-k", pattern,
            "-v" if verbose else "-q",
            "--tb=short",
            f"--junitxml={self.test_results_dir / 'junit-specific.xml'}"
        ]
        
        return self.run_command(command, f"Running tests matching '{pattern}'")
    
    def run_tests_with_markers(self, markers: List[str], verbose: bool = True) -> bool:
        """æ ¹æ®æ ‡è®°è¿è¡Œæµ‹è¯•"""
        marker_expr = " and ".join(markers)
        
        command = [
            sys.executable, "-m", "pytest",
            "-m", marker_expr,
            "-v" if verbose else "-q",
            "--tb=short",
            f"--junitxml={self.test_results_dir / 'junit-markers.xml'}"
        ]
        
        return self.run_command(command, f"Running tests with markers: {marker_expr}")
    
    def generate_coverage_report(self) -> bool:
        """ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š"""
        if not self.coverage_dir.exists():
            print("âŒ No coverage data found. Run tests with coverage first.")
            return False
        
        print(f"\nğŸ“Š Coverage report available at: {self.coverage_dir / 'index.html'}")
        
        # å°è¯•æ‰“å¼€è¦†ç›–ç‡æŠ¥å‘Š
        try:
            import webbrowser
            webbrowser.open(f"file://{self.coverage_dir / 'index.html'}")
            print("ğŸŒ Coverage report opened in browser")
        except Exception:
            print("ğŸ“‚ Open the file manually to view the report")
        
        return True
    
    def clean_test_artifacts(self) -> bool:
        """æ¸…ç†æµ‹è¯•äº§ç”Ÿçš„æ–‡ä»¶"""
        import shutil
        
        artifacts = [
            self.test_results_dir,
            self.coverage_dir,
            self.project_root / ".coverage",
            self.project_root / ".pytest_cache",
            self.project_root / "coverage.xml"
        ]
        
        removed = 0
        for artifact in artifacts:
            if artifact.exists():
                try:
                    if artifact.is_dir():
                        shutil.rmtree(artifact)
                    else:
                        artifact.unlink()
                    removed += 1
                    print(f"ğŸ—‘ï¸ Removed {artifact}")
                except Exception as e:
                    print(f"âš ï¸ Could not remove {artifact}: {e}")
        
        if removed > 0:
            print(f"\nâœ… Cleaned {removed} test artifacts")
        else:
            print("âœ… No test artifacts to clean")
        
        return True
    
    def print_test_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ§ª Vector Storage Test Suite")
        print("="*60)
        print(f"ğŸ“ Project root: {self.project_root}")
        print(f"ğŸ“Š Results dir: {self.test_results_dir}")
        print(f"ğŸ“ˆ Coverage dir: {self.coverage_dir}")
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Vector Storage Test Runner",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/run_tests.py --all                    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
  python scripts/run_tests.py --unit                   # ä»…è¿è¡Œå•å…ƒæµ‹è¯•
  python scripts/run_tests.py --integration            # ä»…è¿è¡Œé›†æˆæµ‹è¯•
  python scripts/run_tests.py --performance            # ä»…è¿è¡Œæ€§èƒ½æµ‹è¯•
  python scripts/run_tests.py --quick                  # è¿è¡Œå¿«é€Ÿæµ‹è¯•
  python scripts/run_tests.py --pattern "faiss"        # è¿è¡ŒåŒ…å«"faiss"çš„æµ‹è¯•
  python scripts/run_tests.py --markers "unit"         # è¿è¡Œæ ‡è®°ä¸ºunitçš„æµ‹è¯•
  python scripts/run_tests.py --clean                  # æ¸…ç†æµ‹è¯•äº§ç”Ÿçš„æ–‡ä»¶
  python scripts/run_tests.py --deps                   # æ£€æŸ¥ä¾èµ–çŠ¶æ€
        """
    )
    
    parser.add_argument("--all", action="store_true", help="è¿è¡Œæ‰€æœ‰æµ‹è¯•")
    parser.add_argument("--unit", action="store_true", help="è¿è¡Œå•å…ƒæµ‹è¯•")
    parser.add_argument("--integration", action="store_true", help="è¿è¡Œé›†æˆæµ‹è¯•")
    parser.add_argument("--performance", action="store_true", help="è¿è¡Œæ€§èƒ½æµ‹è¯•")
    parser.add_argument("--quick", action="store_true", help="è¿è¡Œå¿«é€Ÿæµ‹è¯•")
    
    parser.add_argument("--pattern", type=str, help="è¿è¡ŒåŒ¹é…æ¨¡å¼çš„æµ‹è¯•")
    parser.add_argument("--markers", type=str, nargs="+", help="æ ¹æ®æ ‡è®°è¿è¡Œæµ‹è¯•")
    
    parser.add_argument("--no-coverage", action="store_true", help="ä¸ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š")
    parser.add_argument("--quiet", action="store_true", help="å®‰é™æ¨¡å¼")
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    parser.add_argument("--coverage-report", action="store_true", help="ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š")
    parser.add_argument("--clean", action="store_true", help="æ¸…ç†æµ‹è¯•äº§ç”Ÿçš„æ–‡ä»¶")
    parser.add_argument("--deps", action="store_true", help="æ£€æŸ¥æµ‹è¯•ä¾èµ–")
    
    args = parser.parse_args()
    
    runner = TestRunner()
    runner.print_test_summary()
    
    # å¤„ç†ç‰¹æ®Šå‘½ä»¤
    if args.deps:
        runner.print_dependency_status()
        return
    
    if args.clean:
        runner.clean_test_artifacts()
        return
    
    if args.coverage_report:
        runner.generate_coverage_report()
        return
    
    # æ£€æŸ¥ä¾èµ–
    if not runner.print_dependency_status():
        sys.exit(1)
    
    # è®¾ç½®å‚æ•°
    verbose = not args.quiet
    if args.verbose:
        verbose = True
    
    coverage = not args.no_coverage
    success = True
    
    # è¿è¡Œæµ‹è¯•
    if args.all:
        success = runner.run_all_tests(verbose, coverage)
    elif args.unit:
        success = runner.run_unit_tests(verbose, coverage)
    elif args.integration:
        success = runner.run_integration_tests(verbose, coverage)
    elif args.performance:
        success = runner.run_performance_tests(verbose)
    elif args.quick:
        success = runner.run_quick_tests()
    elif args.pattern:
        success = runner.run_specific_tests(args.pattern, verbose)
    elif args.markers:
        success = runner.run_tests_with_markers(args.markers, verbose)
    else:
        # é»˜è®¤è¿è¡Œå¿«é€Ÿæµ‹è¯•
        print("No specific test type specified, running quick tests...")
        success = runner.run_quick_tests()
    
    # æ‰“å°ç»“æœ
    if success:
        print("\nğŸ‰ All tests completed successfully!")
        if coverage and not args.no_coverage:
            runner.generate_coverage_report()
    else:
        print("\nâŒ Some tests failed!")
        sys.exit(1)


if __name__ == "__main__":
    main()