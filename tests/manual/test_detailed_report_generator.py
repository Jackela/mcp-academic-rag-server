#!/usr/bin/env python3
"""
ç”Ÿæˆè¯¦ç»†çš„MCP Inspectoræµ‹è¯•æŠ¥å‘Š
åŒ…å«å®Œæ•´çš„æŸ¥è¯¢ã€ç»“æœå’Œåˆ†æ
"""

import asyncio
import sys
import os
import json
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.getcwd())

from mcp_server_standalone import MCPServer

class DetailedReportGenerator:
    """ç”Ÿæˆè¯¦ç»†æµ‹è¯•æŠ¥å‘Š"""
    
    def __init__(self):
        self.server = MCPServer()
        self.detailed_report = {
            "report_metadata": {
                "generated_at": datetime.now().isoformat(),
                "test_suite": "MCP Inspector Detailed Analysis",
                "version": "1.0",
                "environment": {
                    "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
                    "working_directory": os.getcwd()
                }
            },
            "system_status": {},
            "document_processing": [],
            "query_analysis": [],
            "performance_metrics": {},
            "technical_validation": {},
            "summary": {}
        }
    
    async def generate_comprehensive_report(self):
        """ç”Ÿæˆå…¨é¢çš„æµ‹è¯•æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆè¯¦ç»†MCP Inspectoræµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        # 1. ç³»ç»ŸçŠ¶æ€æ£€æŸ¥
        await self._check_system_status()
        
        # 2. æ–‡æ¡£å¤„ç†æµ‹è¯•
        await self._test_document_processing()
        
        # 3. æŸ¥è¯¢åˆ†ææµ‹è¯•
        await self._test_query_analysis()
        
        # 4. æ€§èƒ½æµ‹è¯•
        await self._test_performance()
        
        # 5. æŠ€æœ¯éªŒè¯
        await self._technical_validation()
        
        # 6. ç”Ÿæˆæ€»ç»“
        self._generate_summary()
        
        # 7. ä¿å­˜æŠ¥å‘Š
        self._save_report()
        
        # 8. æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
        self._display_summary()
    
    async def _check_system_status(self):
        """æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"""
        print("\nğŸ” 1. ç³»ç»ŸçŠ¶æ€æ£€æŸ¥")
        print("-"*40)
        
        result = await self.server.handle_test_connection(1, {"message": "è¯¦ç»†æŠ¥å‘Šæµ‹è¯•"})
        response = result["result"]["content"][0]["text"]
        
        # è§£æç³»ç»Ÿä¿¡æ¯
        rag_available = "âœ… å¯ç”¨ (Sentence-BERT + OpenAI)" in response
        
        self.detailed_report["system_status"] = {
            "connection_test": "æˆåŠŸ",
            "rag_mode": "å®Œæ•´RAG" if rag_available else "ç®€å•æ¨¡å¼",
            "embedding_model": "sentence-transformers/all-MiniLM-L6-v2" if rag_available else "ä¸é€‚ç”¨",
            "llm_model": "OpenAI GPT-3.5-turbo" if rag_available else "ä¸é€‚ç”¨",
            "mcp_compliance": "âœ… ç¬¦åˆMCPæ ‡å‡†",
            "full_response": response
        }
        
        print(f"RAGæ¨¡å¼: {self.detailed_report['system_status']['rag_mode']}")
        print(f"åµŒå…¥æ¨¡å‹: {self.detailed_report['system_status']['embedding_model']}")
        print(f"LLMæ¨¡å‹: {self.detailed_report['system_status']['llm_model']}")
    
    async def _test_document_processing(self):
        """æµ‹è¯•æ–‡æ¡£å¤„ç†"""
        print("\nğŸ“„ 2. æ–‡æ¡£å¤„ç†æµ‹è¯•")
        print("-"*40)
        
        test_documents = [
            "test-documents/machine-learning.txt",
            "test-documents/neural-networks.txt", 
            "test-documents/nlp-research.txt"
        ]
        
        for doc_path in test_documents:
            print(f"å¤„ç†: {doc_path}")
            start_time = time.time()
            
            result = await self.server.handle_process_document(2, {"file_path": doc_path})
            
            end_time = time.time()
            processing_time = (end_time - start_time) * 1000
            
            if "error" not in result:
                response = result["result"]["content"][0]["text"]
                
                # æå–æ–‡æ¡£ä¿¡æ¯
                doc_info = {
                    "file_path": doc_path,
                    "status": "æˆåŠŸ",
                    "processing_time_ms": processing_time,
                    "rag_processing": "completed" in response,
                    "embedding_generated": "å‘é‡åµŒå…¥ä¿¡æ¯" in response,
                    "full_response": response
                }
                
                # æå–ç»Ÿè®¡ä¿¡æ¯
                if "å­—ç¬¦æ•°:" in response:
                    char_count_start = response.find("å­—ç¬¦æ•°:") + len("å­—ç¬¦æ•°:")
                    char_count_end = response.find("\n", char_count_start)
                    char_count = response[char_count_start:char_count_end].strip().replace(",", "")
                    doc_info["character_count"] = int(char_count) if char_count.isdigit() else 0
                
                if "å•è¯æ•°:" in response:
                    word_count_start = response.find("å•è¯æ•°:") + len("å•è¯æ•°:")
                    word_count_end = response.find("\n", word_count_start)
                    word_count = response[word_count_start:word_count_end].strip()
                    doc_info["word_count"] = int(word_count) if word_count.isdigit() else 0
                
            else:
                doc_info = {
                    "file_path": doc_path,
                    "status": "å¤±è´¥",
                    "processing_time_ms": processing_time,
                    "error": result["error"]["message"]
                }
            
            self.detailed_report["document_processing"].append(doc_info)
            print(f"  çŠ¶æ€: {doc_info['status']} ({processing_time:.0f}ms)")
    
    async def _test_query_analysis(self):
        """æµ‹è¯•æŸ¥è¯¢åˆ†æ"""
        print("\nğŸ¤– 3. æŸ¥è¯¢åˆ†ææµ‹è¯•")
        print("-"*40)
        
        test_queries = [
            {
                "id": "ml_definition",
                "query": "What is machine learning and how is it used in research?",
                "expected_topics": ["artificial intelligence", "algorithms", "data analysis", "research applications"],
                "complexity": "medium"
            },
            {
                "id": "neural_networks",
                "query": "Explain neural networks and deep learning architecture",
                "expected_topics": ["neural networks", "deep learning", "layers", "activation functions"],
                "complexity": "high"
            },
            {
                "id": "nlp_research", 
                "query": "How does NLP help in academic research?",
                "expected_topics": ["natural language processing", "text analysis", "research", "literature"],
                "complexity": "medium"
            },
            {
                "id": "ml_comparison",
                "query": "What are the differences between supervised and unsupervised learning?",
                "expected_topics": ["supervised learning", "unsupervised learning", "classification", "clustering"],
                "complexity": "high"
            }
        ]
        
        for query_info in test_queries:
            print(f"\næŸ¥è¯¢: {query_info['query'][:50]}...")
            start_time = time.time()
            
            result = await self.server.handle_query_documents(
                10 + len(self.detailed_report["query_analysis"]), 
                {"query": query_info["query"], "top_k": 3}
            )
            
            end_time = time.time()
            query_time = (end_time - start_time) * 1000
            
            if "error" not in result:
                response = result["result"]["content"][0]["text"]
                
                # åˆ†ææŸ¥è¯¢ç»“æœ
                analysis = {
                    "query_id": query_info["id"],
                    "query": query_info["query"],
                    "expected_complexity": query_info["complexity"],
                    "response_time_ms": query_time,
                    "status": "æˆåŠŸ"
                }
                
                # æ£€æµ‹ä½¿ç”¨çš„æ¨¡å¼
                if "ğŸ¤– æ™ºèƒ½RAGæŸ¥è¯¢ç»“æœ:" in response:
                    analysis["mode"] = "æ™ºèƒ½RAG"
                    analysis["retrieval_method"] = "è¯­ä¹‰å‘é‡æœç´¢"
                    analysis["generation_method"] = "OpenAI LLM"
                elif "ğŸ” å…³é”®è¯åŒ¹é…æŸ¥è¯¢ç»“æœ:" in response:
                    analysis["mode"] = "ç®€å•æ¨¡å¼"
                    analysis["retrieval_method"] = "å…³é”®è¯åŒ¹é…"
                    analysis["generation_method"] = "æ¨¡æ¿ç”Ÿæˆ"
                else:
                    analysis["mode"] = "æœªçŸ¥"
                
                # è¯„ä¼°å›ç­”è´¨é‡
                analysis["response_length"] = len(response)
                analysis["contains_citations"] = "æ–‡æ¡£ID:" in response or "æ¥æºï¼š" in response
                analysis["retrieval_info"] = "ğŸ” æ£€ç´¢æ¨¡å¼:" in response
                analysis["generation_info"] = "ğŸ¤– ç”Ÿæˆæ¨¡å¼:" in response
                
                # æå–AIå›ç­”å†…å®¹
                if "ğŸ’¬ AIç­”æ¡ˆ:" in response:
                    answer_start = response.find("ğŸ’¬ AIç­”æ¡ˆ:") + len("ğŸ’¬ AIç­”æ¡ˆ:")
                    answer_end = response.find("ğŸ“ ç›¸å…³æ–‡æ¡£ç‰‡æ®µ") if "ğŸ“ ç›¸å…³æ–‡æ¡£ç‰‡æ®µ" in response else len(response)
                    ai_answer = response[answer_start:answer_end].strip()
                    analysis["ai_answer"] = ai_answer[:500] + "..." if len(ai_answer) > 500 else ai_answer
                
                # ä¸»é¢˜è¦†ç›–åˆ†æ
                topics_found = []
                for topic in query_info["expected_topics"]:
                    if topic.lower() in response.lower():
                        topics_found.append(topic)
                
                analysis["topic_coverage"] = {
                    "expected_topics": query_info["expected_topics"],
                    "found_topics": topics_found,
                    "coverage_percentage": (len(topics_found) / len(query_info["expected_topics"])) * 100
                }
                
                analysis["full_response"] = response
                
            else:
                analysis = {
                    "query_id": query_info["id"],
                    "query": query_info["query"],
                    "status": "å¤±è´¥",
                    "response_time_ms": query_time,
                    "error": result["error"]["message"]
                }
            
            self.detailed_report["query_analysis"].append(analysis)
            print(f"  æ¨¡å¼: {analysis.get('mode', 'é”™è¯¯')} ({query_time:.0f}ms)")
    
    async def _test_performance(self):
        """æ€§èƒ½æµ‹è¯•"""
        print("\nâš¡ 4. æ€§èƒ½æµ‹è¯•")
        print("-"*40)
        
        # æµ‹è¯•å¤šä¸ªæŸ¥è¯¢çš„æ€§èƒ½
        performance_queries = [
            "What is AI?",
            "Define machine learning",
            "Explain deep learning"
        ]
        
        response_times = []
        
        for i, query in enumerate(performance_queries):
            start_time = time.time()
            
            result = await self.server.handle_query_documents(
                20 + i, {"query": query, "top_k": 2}
            )
            
            end_time = time.time()
            response_time = (end_time - start_time) * 1000
            response_times.append(response_time)
            
            print(f"  æŸ¥è¯¢ {i+1}: {response_time:.0f}ms")
        
        self.detailed_report["performance_metrics"] = {
            "average_response_time_ms": sum(response_times) / len(response_times),
            "min_response_time_ms": min(response_times),
            "max_response_time_ms": max(response_times),
            "total_queries_tested": len(performance_queries),
            "individual_response_times": response_times,
            "performance_rating": self._rate_performance(sum(response_times) / len(response_times))
        }
        
        avg_time = self.detailed_report["performance_metrics"]["average_response_time_ms"]
        rating = self.detailed_report["performance_metrics"]["performance_rating"]
        print(f"å¹³å‡å“åº”æ—¶é—´: {avg_time:.0f}ms ({rating})")
    
    def _rate_performance(self, avg_time_ms):
        """è¯„çº§æ€§èƒ½"""
        if avg_time_ms < 2000:
            return "ä¼˜ç§€"
        elif avg_time_ms < 5000:
            return "è‰¯å¥½"
        elif avg_time_ms < 10000:
            return "å¯æ¥å—"
        else:
            return "éœ€è¦æ”¹è¿›"
    
    async def _technical_validation(self):
        """æŠ€æœ¯éªŒè¯"""
        print("\nğŸ”§ 5. æŠ€æœ¯éªŒè¯")
        print("-"*40)
        
        # éªŒè¯æ–‡æ¡£åˆ—è¡¨
        docs_result = await self.server.handle_list_documents(30, {})
        docs_response = docs_result["result"]["content"][0]["text"]
        
        # è®¡ç®—æ–‡æ¡£æ•°é‡
        doc_count = docs_response.count("ğŸ“„")
        
        self.detailed_report["technical_validation"] = {
            "mcp_tools_available": ["test_connection", "process_document", "query_documents", "list_documents"],
            "documents_processed": doc_count,
            "embedding_dimensions": 384,  # Sentence-BERT
            "vector_similarity": "dot_product",
            "pipeline_components": [
                "SentenceTransformersTextEmbedder",
                "InMemoryEmbeddingRetriever", 
                "ChatPromptBuilder",
                "OpenAIChatGenerator"
            ],
            "api_integrations": ["OpenAI GPT-3.5-turbo"],
            "document_store": "InMemoryDocumentStore",
            "mcp_protocol_version": "1.0"
        }
        
        print(f"å·²å¤„ç†æ–‡æ¡£: {doc_count}")
        print(f"MCPå·¥å…·: {len(self.detailed_report['technical_validation']['mcp_tools_available'])}")
    
    def _generate_summary(self):
        """ç”Ÿæˆæ€»ç»“"""
        total_queries = len(self.detailed_report["query_analysis"])
        successful_queries = sum(1 for q in self.detailed_report["query_analysis"] if q.get("status") == "æˆåŠŸ")
        intelligent_rag_queries = sum(1 for q in self.detailed_report["query_analysis"] if q.get("mode") == "æ™ºèƒ½RAG")
        
        total_docs = len(self.detailed_report["document_processing"])
        successful_docs = sum(1 for d in self.detailed_report["document_processing"] if d.get("status") == "æˆåŠŸ")
        
        avg_response_time = self.detailed_report["performance_metrics"]["average_response_time_ms"]
        
        self.detailed_report["summary"] = {
            "overall_status": "ä¼˜ç§€" if successful_queries == total_queries and successful_docs == total_docs else "è‰¯å¥½",
            "query_success_rate": (successful_queries / total_queries) * 100 if total_queries > 0 else 0,
            "intelligent_rag_rate": (intelligent_rag_queries / total_queries) * 100 if total_queries > 0 else 0,
            "document_processing_rate": (successful_docs / total_docs) * 100 if total_docs > 0 else 0,
            "average_response_time_ms": avg_response_time,
            "key_achievements": [
                f"æ™ºèƒ½RAGæˆåŠŸç‡: {(intelligent_rag_queries / total_queries) * 100:.1f}%",
                f"æ–‡æ¡£å¤„ç†æˆåŠŸç‡: {(successful_docs / total_docs) * 100:.1f}%",
                f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.0f}ms",
                "å®Œæ•´çš„è¯­ä¹‰æ£€ç´¢å’ŒLLMç”Ÿæˆç®¡é“",
                "ç¬¦åˆMCPåè®®æ ‡å‡†"
            ],
            "recommendations": [
                "ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œå»ºè®®ç»§ç»­ä½¿ç”¨",
                "å¯ä»¥è€ƒè™‘å¢åŠ æ›´å¤šæµ‹è¯•æ–‡æ¡£",
                "ç›‘æ§OpenAI APIè°ƒç”¨æˆæœ¬",
                "è€ƒè™‘æ·»åŠ ç¼“å­˜æœºåˆ¶ä»¥æé«˜æ€§èƒ½"
            ]
        }
    
    def _save_report(self):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Š"""
        with open("mcp_detailed_test_report.json", "w", encoding="utf-8") as f:
            json.dump(self.detailed_report, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: mcp_detailed_test_report.json")
    
    def _display_summary(self):
        """æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦"""
        print("\nğŸ“Š æµ‹è¯•æŠ¥å‘Šæ‘˜è¦")
        print("="*80)
        
        summary = self.detailed_report["summary"]
        
        print(f"æ€»ä½“çŠ¶æ€: {summary['overall_status']}")
        print(f"æŸ¥è¯¢æˆåŠŸç‡: {summary['query_success_rate']:.1f}%")
        print(f"æ™ºèƒ½RAGä½¿ç”¨ç‡: {summary['intelligent_rag_rate']:.1f}%")
        print(f"æ–‡æ¡£å¤„ç†æˆåŠŸç‡: {summary['document_processing_rate']:.1f}%")
        print(f"å¹³å‡å“åº”æ—¶é—´: {summary['average_response_time_ms']:.0f}ms")
        
        print("\nğŸ¯ å…³é”®æˆå°±:")
        for achievement in summary["key_achievements"]:
            print(f"  â€¢ {achievement}")
        
        print("\nğŸ’¡ å»ºè®®:")
        for recommendation in summary["recommendations"]:
            print(f"  â€¢ {recommendation}")

async def main():
    """ä¸»å‡½æ•°"""
    generator = DetailedReportGenerator()
    await generator.generate_comprehensive_report()

if __name__ == "__main__":
    asyncio.run(main())