#!/usr/bin/env python3
"""
MCP Inspector æ¨¡æ‹Ÿæµ‹è¯• - éªŒè¯RAGæ•ˆæœ
æ¨¡æ‹Ÿ MCP Inspector çš„è°ƒè¯•åŠŸèƒ½ï¼Œå¯¹æ¯”ç®€å•æ¨¡å¼vså®Œæ•´RAGæ¨¡å¼
"""

import asyncio
import sys
import os
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.getcwd())

from mcp_server_standalone import MCPServer

class MCPInspectorSimulator:
    """æ¨¡æ‹ŸMCP Inspectorçš„æµ‹è¯•åŠŸèƒ½"""
    
    def __init__(self):
        self.server = MCPServer()
        self.test_results = []
    
    async def simulate_inspector_test(self):
        """æ¨¡æ‹ŸMCP Inspectorçš„å®Œæ•´æµ‹è¯•æµç¨‹"""
        print("ğŸ” MCP Inspector æ¨¡æ‹Ÿæµ‹è¯•")
        print("="*80)
        
        # 1. è¿æ¥æµ‹è¯•
        await self._test_connection()
        
        # 2. å·¥å…·æ¢ç´¢
        await self._explore_tools()
        
        # 3. å¯¹æ¯”æµ‹è¯•ï¼šç®€å•æ¨¡å¼ vs æ™ºèƒ½RAGæ¨¡å¼
        await self._compare_retrieval_modes()
        
        # 4. æ€§èƒ½æµ‹è¯•
        await self._performance_test()
        
        # 5. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self._generate_report()
    
    async def _test_connection(self):
        """æµ‹è¯•è¿æ¥"""
        print("\nğŸ”Œ 1. è¿æ¥æµ‹è¯•")
        print("-"*40)
        
        result = await self.server.handle_test_connection(1, {"message": "Inspectoræµ‹è¯•"})
        response = result["result"]["content"][0]["text"]
        
        # æ£€æŸ¥RAGçŠ¶æ€
        rag_available = "âœ… å¯ç”¨ (Sentence-BERT + OpenAI)" in response
        
        test_result = {
            "test": "connection",
            "status": "âœ… æˆåŠŸ" if rag_available else "âš ï¸ éƒ¨åˆ†åŠŸèƒ½",
            "rag_mode": "å®Œæ•´RAG" if rag_available else "ç®€å•æ¨¡å¼",
            "details": response[:200] + "..." if len(response) > 200 else response
        }
        
        self.test_results.append(test_result)
        print(f"è¿æ¥çŠ¶æ€: {test_result['status']}")
        print(f"RAGæ¨¡å¼: {test_result['rag_mode']}")
    
    async def _explore_tools(self):
        """æ¢ç´¢å¯ç”¨å·¥å…·"""
        print("\nğŸ› ï¸ 2. å·¥å…·æ¢ç´¢")
        print("-"*40)
        
        # æ¨¡æ‹Ÿæ£€æŸ¥å¯ç”¨çš„MCPå·¥å…·
        available_tools = [
            "test_connection", "process_document", 
            "query_documents", "list_documents"
        ]
        
        print("å¯ç”¨å·¥å…·:")
        for tool in available_tools:
            print(f"  â€¢ {tool}")
        
        # æµ‹è¯•æ–‡æ¡£å¤„ç†å·¥å…·
        doc_result = await self.server.handle_process_document(
            2, {"file_path": "test-documents/machine-learning.txt"}
        )
        
        processing_success = "error" not in doc_result
        
        test_result = {
            "test": "tool_exploration",
            "status": "âœ… æˆåŠŸ" if processing_success else "âŒ å¤±è´¥",
            "tools_available": len(available_tools),
            "document_processing": "æˆåŠŸ" if processing_success else "å¤±è´¥"
        }
        
        self.test_results.append(test_result)
        print(f"å·¥å…·æµ‹è¯•: {test_result['status']}")
    
    async def _compare_retrieval_modes(self):
        """å¯¹æ¯”ä¸åŒæ£€ç´¢æ¨¡å¼çš„æ•ˆæœ"""
        print("\nâš–ï¸ 3. æ£€ç´¢æ¨¡å¼å¯¹æ¯”æµ‹è¯•")
        print("-"*40)
        
        test_queries = [
            {
                "query": "What is machine learning?",
                "expected_concepts": ["artificial intelligence", "algorithms", "data", "patterns"]
            },
            {
                "query": "How do neural networks work?",
                "expected_concepts": ["layers", "neurons", "weights", "training"]
            }
        ]
        
        mode_comparison = []
        
        for query_info in test_queries:
            query = query_info["query"]
            print(f"\nğŸ” æŸ¥è¯¢: {query}")
            
            # æ‰§è¡ŒæŸ¥è¯¢
            result = await self.server.handle_query_documents(
                3, {"query": query, "top_k": 2}
            )
            
            if "error" not in result:
                response = result["result"]["content"][0]["text"]
                
                # æ£€æŸ¥ä½¿ç”¨çš„æ¨¡å¼
                if "ğŸ¤– æ™ºèƒ½RAGæŸ¥è¯¢ç»“æœ:" in response:
                    mode = "æ™ºèƒ½RAG"
                    quality = "é«˜"
                elif "ğŸ” å…³é”®è¯åŒ¹é…æŸ¥è¯¢ç»“æœ:" in response:
                    mode = "ç®€å•æ¨¡å¼"
                    quality = "ä¸­"
                else:
                    mode = "æœªçŸ¥"
                    quality = "ä½"
                
                # è¯„ä¼°å›ç­”è´¨é‡
                response_quality = self._evaluate_response_quality(
                    response, query_info["expected_concepts"]
                )
                
                comparison_result = {
                    "query": query,
                    "mode": mode,
                    "quality": quality,
                    "response_length": len(response),
                    "concept_coverage": response_quality,
                    "status": "âœ… æˆåŠŸ"
                }
                
                print(f"  æ¨¡å¼: {mode}")
                print(f"  è´¨é‡: {quality}")
                print(f"  æ¦‚å¿µè¦†ç›–: {response_quality}%")
                
            else:
                comparison_result = {
                    "query": query,
                    "mode": "é”™è¯¯",
                    "quality": "ä½",
                    "status": "âŒ å¤±è´¥",
                    "error": result["error"]["message"]
                }
                print(f"  çŠ¶æ€: æŸ¥è¯¢å¤±è´¥")
            
            mode_comparison.append(comparison_result)
        
        test_result = {
            "test": "mode_comparison",
            "comparisons": mode_comparison,
            "intelligent_rag_success_rate": sum(1 for c in mode_comparison if c["mode"] == "æ™ºèƒ½RAG") / len(mode_comparison) * 100
        }
        
        self.test_results.append(test_result)
        
        print(f"\næ™ºèƒ½RAGæˆåŠŸç‡: {test_result['intelligent_rag_success_rate']:.1f}%")
    
    def _evaluate_response_quality(self, response, expected_concepts):
        """è¯„ä¼°å›ç­”è´¨é‡"""
        found_concepts = 0
        for concept in expected_concepts:
            if concept.lower() in response.lower():
                found_concepts += 1
        
        return (found_concepts / len(expected_concepts)) * 100 if expected_concepts else 0
    
    async def _performance_test(self):
        """æ€§èƒ½æµ‹è¯•"""
        print("\nâš¡ 4. æ€§èƒ½æµ‹è¯•")
        print("-"*40)
        
        import time
        
        # æµ‹è¯•æŸ¥è¯¢å“åº”æ—¶é—´
        start_time = time.time()
        result = await self.server.handle_query_documents(
            4, {"query": "What is deep learning?", "top_k": 3}
        )
        end_time = time.time()
        
        response_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        performance_result = {
            "test": "performance",
            "response_time_ms": response_time,
            "status": "âœ… ä¼˜ç§€" if response_time < 5000 else ("âš ï¸ å¯æ¥å—" if response_time < 10000 else "âŒ æ…¢"),
            "openai_calls": "æˆåŠŸ" if "error" not in result else "å¤±è´¥"
        }
        
        self.test_results.append(performance_result)
        
        print(f"æŸ¥è¯¢å“åº”æ—¶é—´: {response_time:.0f}ms")
        print(f"æ€§èƒ½è¯„çº§: {performance_result['status']}")
    
    def _generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\nğŸ“Š 5. MCP Inspector æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        # ç»Ÿè®¡ç»“æœ
        total_tests = len(self.test_results)
        successful_tests = sum(1 for r in self.test_results if "âœ…" in str(r.get("status", "")))
        
        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"æˆåŠŸæµ‹è¯•: {successful_tests}")
        print(f"æˆåŠŸç‡: {successful_tests/total_tests*100:.1f}%")
        
        print("\nè¯¦ç»†ç»“æœ:")
        for i, result in enumerate(self.test_results, 1):
            print(f"\n{i}. {result['test'].replace('_', ' ').title()}")
            print(f"   çŠ¶æ€: {result.get('status', 'N/A')}")
            
            if result['test'] == 'connection':
                print(f"   RAGæ¨¡å¼: {result.get('rag_mode', 'N/A')}")
            elif result['test'] == 'mode_comparison':
                print(f"   æ™ºèƒ½RAGæˆåŠŸç‡: {result.get('intelligent_rag_success_rate', 0):.1f}%")
            elif result['test'] == 'performance':
                print(f"   å“åº”æ—¶é—´: {result.get('response_time_ms', 0):.0f}ms")
        
        # æ€»ä½“è¯„ä¼°
        print(f"\nğŸ¯ æ€»ä½“è¯„ä¼°:")
        if successful_tests == total_tests:
            print("ğŸŸ¢ ä¼˜ç§€ - æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ŒRAGç³»ç»Ÿè¿è¡Œå®Œç¾")
        elif successful_tests >= total_tests * 0.8:
            print("ğŸŸ¡ è‰¯å¥½ - å¤§å¤šæ•°æµ‹è¯•é€šè¿‡ï¼Œç³»ç»ŸåŸºæœ¬æ­£å¸¸")
        else:
            print("ğŸ”´ éœ€è¦æ”¹è¿› - å¤šä¸ªæµ‹è¯•å¤±è´¥ï¼Œç³»ç»Ÿå­˜åœ¨é—®é¢˜")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        with open("mcp_inspector_test_report.json", "w", encoding="utf-8") as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: mcp_inspector_test_report.json")

async def main():
    """ä¸»å‡½æ•°"""
    inspector = MCPInspectorSimulator()
    await inspector.simulate_inspector_test()

if __name__ == "__main__":
    asyncio.run(main())