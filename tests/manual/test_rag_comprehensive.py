#!/usr/bin/env python3
"""
Comprehensive RAG Pipeline Test
éªŒè¯å®Œæ•´çš„RAGåŠŸèƒ½ï¼šæ–‡æ¡£å¤„ç† -> åµŒå…¥ç”Ÿæˆ -> è¯­ä¹‰æ£€ç´¢ -> LLMç”Ÿæˆç­”æ¡ˆ
"""

import json
import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.getcwd())

# å¯¼å…¥æœåŠ¡å™¨
from mcp_server_standalone import MCPServer

async def test_rag_pipeline():
    """æµ‹è¯•å®Œæ•´RAGç®¡é“åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹RAGç®¡é“ç»¼åˆæµ‹è¯•")
    print("=" * 80)
    
    # åˆå§‹åŒ–æœåŠ¡å™¨
    server = MCPServer()
    
    # æµ‹è¯•1: æ£€æŸ¥RAGçŠ¶æ€
    print("\nğŸ” æµ‹è¯•1: æ£€æŸ¥RAGåˆå§‹åŒ–çŠ¶æ€")
    connection_result = await server.handle_test_connection(1, {"message": "RAGæµ‹è¯•"})
    response_text = connection_result["result"]["content"][0]["text"]
    print("Response:", response_text)
    
    rag_available = "âœ… å¯ç”¨ (Sentence-BERT + OpenAI)" in response_text
    print(f"RAGçŠ¶æ€: {'âœ… å®Œæ•´RAGå¯ç”¨' if rag_available else 'âŒ ä»…å…³é”®è¯æ¨¡å¼'}")
    
    if not rag_available:
        print("âš ï¸ å®Œæ•´RAGä¸å¯ç”¨ï¼Œæ£€æŸ¥APIå¯†é’¥å’Œä¾èµ–")
        return
    
    # æµ‹è¯•2: å¤„ç†å¤šä¸ªæ–‡æ¡£
    print("\nğŸ“„ æµ‹è¯•2: å¤„ç†æµ‹è¯•æ–‡æ¡£")
    test_docs = [
        "test-documents/machine-learning.txt",
        "test-documents/neural-networks.txt", 
        "test-documents/nlp-research.txt"
    ]
    
    for doc_path in test_docs:
        print(f"\nå¤„ç†æ–‡æ¡£: {doc_path}")
        result = await server.handle_process_document(2, {"file_path": doc_path})
        
        if "error" in result:
            print(f"âŒ å¤„ç†å¤±è´¥: {result['error']['message']}")
            continue
            
        response = result["result"]["content"][0]["text"]
        print("å¤„ç†ç»“æœ:")
        print(response[:500] + "..." if len(response) > 500 else response)
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨å®Œæ•´RAG
        if "RAGå¤„ç†çŠ¶æ€: completed" in response:
            print("âœ… ä½¿ç”¨å®Œæ•´RAGç®¡é“å¤„ç†")
        else:
            print("âŒ æœªä½¿ç”¨å®Œæ•´RAGç®¡é“")
    
    # æµ‹è¯•3: æ™ºèƒ½æŸ¥è¯¢æµ‹è¯•
    print("\nğŸ¤– æµ‹è¯•3: æ™ºèƒ½RAGæŸ¥è¯¢")
    
    test_queries = [
        {
            "query": "What is machine learning and how is it used in research?",
            "expected": "åº”è¯¥æä¾›MLçš„å®šä¹‰å’Œç ”ç©¶åº”ç”¨"
        },
        {
            "query": "Explain neural networks and deep learning",
            "expected": "åº”è¯¥è§£é‡Šç¥ç»ç½‘ç»œæ¶æ„å’Œæ·±åº¦å­¦ä¹ "
        },
        {
            "query": "How does NLP help in academic research?",
            "expected": "åº”è¯¥è¯´æ˜NLPåœ¨å­¦æœ¯ç ”ç©¶ä¸­çš„åº”ç”¨"
        },
        {
            "query": "What are the differences between supervised and unsupervised learning?",
            "expected": "åº”è¯¥å¯¹æ¯”ä¸åŒå­¦ä¹ ç±»å‹"
        }
    ]
    
    for i, test in enumerate(test_queries, 1):
        print(f"\næŸ¥è¯¢ {i}: {test['query']}")
        print(f"æœŸæœ›: {test['expected']}")
        
        result = await server.handle_query_documents(3, {
            "query": test["query"],
            "top_k": 3
        })
        
        if "error" in result:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {result['error']['message']}")
            continue
            
        response = result["result"]["content"][0]["text"]
        print("\næŸ¥è¯¢ç»“æœ:")
        print(response)
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ™ºèƒ½RAG
        if "ğŸ¤– æ™ºèƒ½RAGæŸ¥è¯¢ç»“æœ:" in response:
            print("âœ… ä½¿ç”¨æ™ºèƒ½RAGæŸ¥è¯¢")
        elif "ğŸ” å…³é”®è¯åŒ¹é…æŸ¥è¯¢ç»“æœ:" in response:
            print("âš ï¸ å›é€€åˆ°å…³é”®è¯åŒ¹é…")
        else:
            print("â“ æœªçŸ¥æŸ¥è¯¢æ¨¡å¼")
        
        print("-" * 60)
    
    # æµ‹è¯•4: æ£€æŸ¥æ–‡æ¡£åˆ—è¡¨
    print("\nğŸ“š æµ‹è¯•4: æŸ¥çœ‹å¤„ç†çš„æ–‡æ¡£")
    docs_result = await server.handle_list_documents(4, {})
    docs_response = docs_result["result"]["content"][0]["text"]
    print(docs_response)
    
    print("\nğŸ‰ RAGç®¡é“æµ‹è¯•å®Œæˆ!")

def main():
    """ä¸»å‡½æ•°"""
    asyncio.run(test_rag_pipeline())

if __name__ == "__main__":
    main()