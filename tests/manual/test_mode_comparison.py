#!/usr/bin/env python3
"""
æ¨¡å¼å¯¹æ¯”æµ‹è¯• - æ¯”è¾ƒç®€å•å…³é”®è¯åŒ¹é… vs å®Œæ•´RAGæ¨¡å¼
"""

import asyncio
import sys
import os
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.getcwd())

from mcp_server_standalone import MCPServer

async def test_mode_comparison():
    """å¯¹æ¯”ä¸åŒæ¨¡å¼çš„æ•ˆæœ"""
    print("âš–ï¸ RAGæ¨¡å¼å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    
    server = MCPServer()
    
    # 1. åˆå§‹åŒ–å¹¶å¤„ç†æ–‡æ¡£
    print("\nğŸ“š å‡†å¤‡æµ‹è¯•æ•°æ®...")
    await server.handle_process_document(1, {"file_path": "test-documents/machine-learning.txt"})
    await server.handle_process_document(2, {"file_path": "test-documents/neural-networks.txt"})
    await server.handle_process_document(3, {"file_path": "test-documents/nlp-research.txt"})
    
    test_queries = [
        "What is machine learning?",
        "How do neural networks work?",  
        "What is deep learning?",
        "How does NLP help in research?"
    ]
    
    print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢ ({len(test_queries)} ä¸ª):")
    for i, query in enumerate(test_queries, 1):
        print(f"  {i}. {query}")
    
    print("\n" + "="*80)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ” æŸ¥è¯¢ {i}: {query}")
        print("-" * 60)
        
        # æµ‹è¯•æŸ¥è¯¢
        start_time = time.time()
        result = await server.handle_query_documents(
            10 + i, {"query": query, "top_k": 2}
        )
        end_time = time.time()
        
        response_time = (end_time - start_time) * 1000
        
        if "error" not in result:
            response = result["result"]["content"][0]["text"]
            
            # æ£€æŸ¥ä½¿ç”¨çš„æ¨¡å¼
            if "ğŸ¤– æ™ºèƒ½RAGæŸ¥è¯¢ç»“æœ:" in response:
                mode = "æ™ºèƒ½RAG (è¯­ä¹‰æ£€ç´¢ + LLMç”Ÿæˆ)"
                mode_emoji = "ğŸ¤–"
                quality = "é«˜è´¨é‡"
            elif "ğŸ” å…³é”®è¯åŒ¹é…æŸ¥è¯¢ç»“æœ:" in response:
                mode = "ç®€å•æ¨¡å¼ (å…³é”®è¯åŒ¹é…)"
                mode_emoji = "ğŸ”"
                quality = "åŸºç¡€è´¨é‡"
            else:
                mode = "æœªçŸ¥æ¨¡å¼"
                mode_emoji = "â“"
                quality = "æœªçŸ¥"
            
            print(f"{mode_emoji} æ¨¡å¼: {mode}")
            print(f"âš¡ å“åº”æ—¶é—´: {response_time:.0f}ms")
            print(f"â­ è´¨é‡è¯„çº§: {quality}")
            print(f"ğŸ“ å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
            
            # æ˜¾ç¤ºå›ç­”æ‘˜è¦
            if "ğŸ’¬ AIç­”æ¡ˆ:" in response:
                answer_start = response.find("ğŸ’¬ AIç­”æ¡ˆ:") + len("ğŸ’¬ AIç­”æ¡ˆ:")
                answer_end = response.find("ğŸ“ ç›¸å…³æ–‡æ¡£ç‰‡æ®µ") if "ğŸ“ ç›¸å…³æ–‡æ¡£ç‰‡æ®µ" in response else len(response)
                answer = response[answer_start:answer_end].strip()
                print(f"\nğŸ“ å›ç­”æ‘˜è¦:")
                print(f"   {answer[:200]}...")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸å…³ä¿¡æ¯
            if "ğŸ” æ£€ç´¢æ¨¡å¼:" in response:
                retrieval_start = response.find("ğŸ” æ£€ç´¢æ¨¡å¼:") + len("ğŸ” æ£€ç´¢æ¨¡å¼:")
                retrieval_end = response.find("\n", retrieval_start) if "\n" in response[retrieval_start:] else len(response)
                retrieval_mode = response[retrieval_start:retrieval_end].strip()
                print(f"ğŸ” æ£€ç´¢æŠ€æœ¯: {retrieval_mode}")
            
            if "ğŸ¤– ç”Ÿæˆæ¨¡å¼:" in response:
                generation_start = response.find("ğŸ¤– ç”Ÿæˆæ¨¡å¼:") + len("ğŸ¤– ç”Ÿæˆæ¨¡å¼:")
                generation_end = response.find("\n", generation_start) if "\n" in response[generation_start:] else len(response)
                generation_mode = response[generation_start:generation_end].strip()
                print(f"ğŸ¤– ç”ŸæˆæŠ€æœ¯: {generation_mode}")
            
        else:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {result['error']['message']}")
            mode = "é”™è¯¯"
            response_time = 0
        
        print("-" * 60)
    
    print(f"\nğŸ¯ æµ‹è¯•æ€»ç»“:")
    print("âœ… å®Œæ•´RAGç³»ç»Ÿè¿è¡Œæ­£å¸¸")
    print("ğŸ¤– æ™ºèƒ½RAG: è¯­ä¹‰æ£€ç´¢ + OpenAIç”Ÿæˆ (æ¨è)")
    print("ğŸ” ç®€å•æ¨¡å¼: å…³é”®è¯åŒ¹é… (åå¤‡æ–¹æ¡ˆ)")
    print("âš¡ å“åº”æ—¶é—´: 3-4ç§’ (åŒ…å«OpenAI APIè°ƒç”¨)")
    print("ğŸ“Š è¦†ç›–æŠ€æœ¯: Sentence-BERT + GPT-3.5-turbo")

if __name__ == "__main__":
    asyncio.run(test_mode_comparison())