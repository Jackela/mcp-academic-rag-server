"""
çŸ¥è¯†å›¾è°±å¤„ç†ç¤ºä¾‹è„šæœ¬

è¯¥è„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•åœ¨æ–‡æ¡£å¤„ç†æµæ°´çº¿ä¸­é›†æˆå’Œä½¿ç”¨çŸ¥è¯†å›¾è°±å¤„ç†å™¨ï¼ŒåŒ…æ‹¬ï¼š
- åˆ›å»ºåŒ…å«çŸ¥è¯†å›¾è°±å¤„ç†å™¨çš„å¤„ç†æµæ°´çº¿
- å¤„ç†å­¦æœ¯æ–‡æ¡£å¹¶æå–çŸ¥è¯†å›¾è°±
- æŸ¥çœ‹å’Œåˆ†ææå–çš„å®ä½“ã€å…³ç³»å’Œæ¦‚å¿µ
- å¯¼å‡ºçŸ¥è¯†å›¾è°±æ•°æ®

ç”¨æ³•ï¼š
    python knowledge_graph_example.py [config_path]
"""

import os
import sys
import json
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.pipeline import Pipeline
from models.document import Document
from processors.pre_processor import PreProcessor
from processors.ocr_processor import OCRProcessor
from processors.structure_processor import StructureProcessor
from processors.knowledge_graph_processor import KnowledgeGraphProcessor
from core.config_manager import ConfigManager


def create_knowledge_graph_pipeline(config_path: str) -> Pipeline:
    """
    åˆ›å»ºåŒ…å«çŸ¥è¯†å›¾è°±å¤„ç†å™¨çš„å¤„ç†æµæ°´çº¿
    
    Args:
        config_path (str): é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        Pipeline: é…ç½®å¥½çš„å¤„ç†æµæ°´çº¿
    """
    print("åˆ›å»ºçŸ¥è¯†å›¾è°±å¤„ç†æµæ°´çº¿...")
    
    # åŠ è½½é…ç½®
    config_manager = ConfigManager(config_path)
    config = config_manager.get_config()
    
    # åˆ›å»ºæµæ°´çº¿
    pipeline = Pipeline("KnowledgeGraphPipeline")
    
    # æ·»åŠ é¢„å¤„ç†å™¨
    if config.get("processors", {}).get("pre_processor", {}).get("enabled", False):
        pre_config = config["processors"]["pre_processor"]["config"]
        pre_processor = PreProcessor(pre_config)
        pipeline.add_processor(pre_processor)
        print("âœ“ å·²æ·»åŠ é¢„å¤„ç†å™¨")
    
    # æ·»åŠ OCRå¤„ç†å™¨
    if config.get("processors", {}).get("ocr_processor", {}).get("enabled", False):
        ocr_config = config["processors"]["ocr_processor"]["config"]
        ocr_processor = OCRProcessor(ocr_config)
        pipeline.add_processor(ocr_processor)
        print("âœ“ å·²æ·»åŠ OCRå¤„ç†å™¨")
    
    # æ·»åŠ ç»“æ„å¤„ç†å™¨
    if config.get("processors", {}).get("structure_processor", {}).get("enabled", False):
        structure_config = config["processors"]["structure_processor"]["config"]
        structure_processor = StructureProcessor(structure_config)
        pipeline.add_processor(structure_processor)
        print("âœ“ å·²æ·»åŠ ç»“æ„å¤„ç†å™¨")
    
    # æ·»åŠ çŸ¥è¯†å›¾è°±å¤„ç†å™¨
    if config.get("processors", {}).get("knowledge_graph_processor", {}).get("enabled", False):
        kg_config = config["processors"]["knowledge_graph_processor"]["config"]
        kg_processor = KnowledgeGraphProcessor(kg_config)
        pipeline.add_processor(kg_processor)
        print("âœ“ å·²æ·»åŠ çŸ¥è¯†å›¾è°±å¤„ç†å™¨")
    
    print(f"æµæ°´çº¿åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(pipeline.get_processors())} ä¸ªå¤„ç†å™¨\n")
    return pipeline


def create_sample_document() -> Document:
    """
    åˆ›å»ºæ ·æœ¬å­¦æœ¯æ–‡æ¡£
    
    Returns:
        Document: æ ·æœ¬æ–‡æ¡£å¯¹è±¡
    """
    # åˆ›å»ºåŒ…å«å­¦æœ¯å†…å®¹çš„æ ·æœ¬æ–‡æ¡£
    sample_content = """
    Machine Learning Approaches for Natural Language Processing
    
    Abstract
    This paper presents a comprehensive study on machine learning approaches for natural language processing tasks. 
    We compare various neural network architectures including convolutional neural networks, recurrent neural networks, 
    and transformer models. Our experiments demonstrate that transformer-based models achieve superior performance 
    on text classification and sentiment analysis tasks.
    
    1. Introduction
    Natural language processing (NLP) has witnessed significant advancements with the introduction of deep learning 
    techniques. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been widely used 
    for various NLP tasks. Recently, transformer architectures have revolutionized the field by introducing 
    attention mechanisms that enable better understanding of contextual relationships.
    
    2. Related Work
    Smith et al. (2020) proposed a novel CNN architecture for text classification. Johnson and Brown (2021) 
    developed an improved RNN model for sentiment analysis. The transformer model introduced by Vaswani et al. (2017) 
    has become the foundation for many state-of-the-art NLP systems.
    
    3. Methodology
    Our approach combines multiple machine learning algorithms to create an ensemble model. We use feature extraction 
    techniques to process textual data and apply dimensionality reduction to improve computational efficiency. 
    The proposed method achieves 95% accuracy on the benchmark dataset.
    
    Table 1: Performance Comparison
    Model       Accuracy    Precision   Recall
    CNN         0.85        0.82        0.87
    RNN         0.88        0.86        0.89
    Transformer 0.95        0.94        0.96
    
    Figure 1: Architecture Diagram
    The figure shows the overall system architecture with three main components: feature extraction, 
    model training, and evaluation.
    
    4. Conclusion
    In this work, we demonstrated that transformer-based approaches outperform traditional CNN and RNN models 
    for NLP tasks. The attention mechanism enables better capture of long-range dependencies in text.
    
    References
    [1] Smith, J., et al. (2020). Advanced CNN architectures for text processing. Journal of Machine Learning.
    [2] Johnson, A., Brown, B. (2021). Improved RNN models for sentiment analysis. Conference on NLP.
    [3] Vaswani, A., et al. (2017). Attention is all you need. NIPS.
    """
    
    # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
    doc = Document("sample_academic_paper.txt")
    doc.file_type = "text"
    doc.file_path = "sample_academic_paper.txt"
    
    # æ¨¡æ‹ŸOCRå¤„ç†ç»“æœ
    ocr_result = {
        "text": sample_content,
        "pages": [{"page_num": 1, "text": sample_content}],
        "text_by_page": [sample_content]
    }
    doc.store_content("ocr", ocr_result)
    
    return doc


def analyze_knowledge_graph(kg_data: dict) -> None:
    """
    åˆ†æå’Œå±•ç¤ºçŸ¥è¯†å›¾è°±æ•°æ®
    
    Args:
        kg_data (dict): çŸ¥è¯†å›¾è°±æ•°æ®
    """
    print("çŸ¥è¯†å›¾è°±åˆ†æç»“æœ")
    print("=" * 60)
    
    knowledge_graph = kg_data.get("knowledge_graph", {})
    statistics = kg_data.get("statistics", {})
    
    # å±•ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  å®ä½“æ€»æ•°: {statistics.get('total_entities', 0)}")
    print(f"  å…³ç³»æ€»æ•°: {statistics.get('total_relations', 0)}")
    print(f"  æ¦‚å¿µæ€»æ•°: {statistics.get('total_concepts', 0)}")
    print(f"  å¹³å‡å®ä½“ç½®ä¿¡åº¦: {statistics.get('avg_entity_confidence', 0):.3f}")
    print(f"  å¹³å‡å…³ç³»ç½®ä¿¡åº¦: {statistics.get('avg_relation_confidence', 0):.3f}")
    
    # å±•ç¤ºå®ä½“ç±»å‹åˆ†å¸ƒ
    entity_types = statistics.get('entity_types', {})
    if entity_types:
        print("\nğŸ·ï¸  å®ä½“ç±»å‹åˆ†å¸ƒ:")
        for entity_type, count in entity_types.items():
            print(f"  {entity_type}: {count}")
    
    # å±•ç¤ºå…³ç³»ç±»å‹åˆ†å¸ƒ
    relation_types = statistics.get('relation_types', {})
    if relation_types:
        print("\nğŸ”— å…³ç³»ç±»å‹åˆ†å¸ƒ:")
        for relation_type, count in relation_types.items():
            print(f"  {relation_type}: {count}")
    
    # å±•ç¤ºä¸»è¦å®ä½“
    entities = knowledge_graph.get("entities", {})
    if entities:
        print("\nğŸ¯ ä¸»è¦å®ä½“ (Top 10):")
        sorted_entities = sorted(
            entities.items(),
            key=lambda x: (x[1].get('confidence', 0), x[1].get('frequency', 0)),
            reverse=True
        )
        
        for i, (entity_name, entity_info) in enumerate(sorted_entities[:10], 1):
            entity_type = entity_info.get('type', 'UNKNOWN')
            confidence = entity_info.get('confidence', 0)
            frequency = entity_info.get('frequency', 0)
            print(f"  {i:2d}. {entity_name} ({entity_type}) - ç½®ä¿¡åº¦: {confidence:.3f}, é¢‘ç‡: {frequency}")
    
    # å±•ç¤ºä¸»è¦å…³ç³»
    relations = knowledge_graph.get("relations", [])
    if relations:
        print("\nğŸ”„ ä¸»è¦å…³ç³» (Top 10):")
        sorted_relations = sorted(relations, key=lambda x: x.get('confidence', 0), reverse=True)
        
        for i, relation in enumerate(sorted_relations[:10], 1):
            subject = relation.get('subject', '')
            predicate = relation.get('predicate', '')
            obj = relation.get('object', '')
            confidence = relation.get('confidence', 0)
            print(f"  {i:2d}. {subject} --[{predicate}]--> {obj} (ç½®ä¿¡åº¦: {confidence:.3f})")
    
    # å±•ç¤ºæ¦‚å¿µ
    concepts = knowledge_graph.get("concepts", [])
    if concepts:
        print("\nğŸ’¡ æå–çš„æ¦‚å¿µ (Top 5):")
        sorted_concepts = sorted(concepts, key=lambda x: x.get('confidence', 0), reverse=True)
        
        for i, concept in enumerate(sorted_concepts[:5], 1):
            concept_name = concept.get('name', '')
            confidence = concept.get('confidence', 0)
            print(f"  {i}. {concept_name} (ç½®ä¿¡åº¦: {confidence:.3f})")


def export_knowledge_graph(kg_data: dict, output_path: str) -> None:
    """
    å¯¼å‡ºçŸ¥è¯†å›¾è°±æ•°æ®åˆ°æ–‡ä»¶
    
    Args:
        kg_data (dict): çŸ¥è¯†å›¾è°±æ•°æ®
        output_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    print(f"\nğŸ“„ å¯¼å‡ºçŸ¥è¯†å›¾è°±æ•°æ®åˆ°: {output_path}")
    
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(kg_data, f, ensure_ascii=False, indent=2)
        print("âœ“ å¯¼å‡ºæˆåŠŸ")
    except Exception as e:
        print(f"âœ— å¯¼å‡ºå¤±è´¥: {e}")


def generate_cypher_queries(kg_data: dict) -> List[str]:
    """
    ç”Ÿæˆç”¨äºå›¾æ•°æ®åº“çš„CypheræŸ¥è¯¢è¯­å¥
    
    Args:
        kg_data (dict): çŸ¥è¯†å›¾è°±æ•°æ®
        
    Returns:
        List[str]: CypheræŸ¥è¯¢è¯­å¥åˆ—è¡¨
    """
    queries = []
    knowledge_graph = kg_data.get("knowledge_graph", {})
    
    # åˆ›å»ºå®ä½“èŠ‚ç‚¹
    entities = knowledge_graph.get("entities", {})
    for entity_name, entity_info in entities.items():
        entity_type = entity_info.get('type', 'Entity')
        confidence = entity_info.get('confidence', 0)
        frequency = entity_info.get('frequency', 0)
        
        # è½¬ä¹‰å®ä½“åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        escaped_name = entity_name.replace("'", "\\'").replace('"', '\\"')
        
        query = f"""CREATE (:{entity_type} {{
    name: '{escaped_name}',
    confidence: {confidence},
    frequency: {frequency}
}})"""
        queries.append(query)
    
    # åˆ›å»ºå…³ç³»
    relations = knowledge_graph.get("relations", [])
    for relation in relations:
        subject = relation.get('subject', '').replace("'", "\\'")
        predicate = relation.get('predicate', 'RELATED_TO')
        obj = relation.get('object', '').replace("'", "\\'")
        confidence = relation.get('confidence', 0)
        
        query = f"""MATCH (a {{name: '{subject}'}}), (b {{name: '{obj}'}})
CREATE (a)-[:{predicate} {{confidence: {confidence}}}]->(b)"""
        queries.append(query)
    
    return queries


def run_knowledge_graph_example(config_path: str) -> None:
    """
    è¿è¡ŒçŸ¥è¯†å›¾è°±å¤„ç†ç¤ºä¾‹
    
    Args:
        config_path (str): é…ç½®æ–‡ä»¶è·¯å¾„
    """
    print("çŸ¥è¯†å›¾è°±å¤„ç†ç¤ºä¾‹")
    print("=" * 80 + "\n")
    
    try:
        # 1. åˆ›å»ºå¤„ç†æµæ°´çº¿
        pipeline = create_knowledge_graph_pipeline(config_path)
        
        # 2. åˆ›å»ºæ ·æœ¬æ–‡æ¡£
        print("åˆ›å»ºæ ·æœ¬å­¦æœ¯æ–‡æ¡£...")
        document = create_sample_document()
        print("âœ“ æ ·æœ¬æ–‡æ¡£åˆ›å»ºå®Œæˆ\n")
        
        # 3. æ¨¡æ‹Ÿç»“æ„å¤„ç†ç»“æœï¼ˆé€šå¸¸ç”±StructureProcessorç”Ÿæˆï¼‰
        print("æ¨¡æ‹Ÿæ–‡æ¡£ç»“æ„å¤„ç†...")
        structure_result = {
            "structure": {
                "title": "Machine Learning Approaches for Natural Language Processing",
                "abstract": "This paper presents a comprehensive study on machine learning approaches for natural language processing tasks...",
                "sections": [
                    {
                        "title": "Introduction",
                        "content": "Natural language processing (NLP) has witnessed significant advancements..."
                    },
                    {
                        "title": "Methodology", 
                        "content": "Our approach combines multiple machine learning algorithms..."
                    }
                ]
            }
        }
        document.store_content("structure_recognition", structure_result)
        print("âœ“ ç»“æ„å¤„ç†å®Œæˆ\n")
        
        # 4. è¿è¡ŒçŸ¥è¯†å›¾è°±å¤„ç†å™¨
        print("æ‰§è¡ŒçŸ¥è¯†å›¾è°±æå–...")
        kg_processor = KnowledgeGraphProcessor()
        result = kg_processor.process(document)
        
        if result.is_successful():
            print("âœ“ çŸ¥è¯†å›¾è°±æå–æˆåŠŸ\n")
            
            # 5. åˆ†æç»“æœ
            kg_data = result.get_data()
            analyze_knowledge_graph(kg_data)
            
            # 6. å¯¼å‡ºç»“æœ
            output_dir = os.path.join(os.path.dirname(__file__), "output")
            os.makedirs(output_dir, exist_ok=True)
            
            output_path = os.path.join(output_dir, "knowledge_graph.json")
            export_knowledge_graph(kg_data, output_path)
            
            # 7. ç”ŸæˆCypheræŸ¥è¯¢ç¤ºä¾‹
            print("\nğŸ” ç”Ÿæˆçš„CypheræŸ¥è¯¢ç¤ºä¾‹ (å‰5ä¸ª):")
            cypher_queries = generate_cypher_queries(kg_data)
            for i, query in enumerate(cypher_queries[:5], 1):
                print(f"\næŸ¥è¯¢ {i}:")
                print(query)
            
            cypher_path = os.path.join(output_dir, "cypher_queries.cql")
            with open(cypher_path, 'w', encoding='utf-8') as f:
                f.write('\n\n'.join(cypher_queries))
            print(f"\nâœ“ å®Œæ•´çš„CypheræŸ¥è¯¢å·²ä¿å­˜åˆ°: {cypher_path}")
            
        else:
            print(f"âœ— çŸ¥è¯†å›¾è°±æå–å¤±è´¥: {result.get_message()}")
    
    except Exception as e:
        print(f"âœ— ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        print("\n" + "=" * 80)
        print("çŸ¥è¯†å›¾è°±å¤„ç†ç¤ºä¾‹å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="çŸ¥è¯†å›¾è°±å¤„ç†ç¤ºä¾‹è„šæœ¬")
    parser.add_argument("config_path", nargs="?", default="./config/config.json", 
                        help="é…ç½®æ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()
    
    run_knowledge_graph_example(args.config_path)


if __name__ == "__main__":
    main()