#!/usr/bin/env python3
"""
MCP Academic RAG Server - å‘é‡å­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½æµ‹è¯•
éªŒè¯FAISSã€Memoryå‘é‡å­˜å‚¨å’Œè¯­ä¹‰æ£€ç´¢åŠŸèƒ½
"""

import sys
import os
import asyncio
import logging
import signal
import atexit
import json
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
sys.path.insert(0, os.path.abspath('.'))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VectorStorageRetrievalTester:
    """å‘é‡å­˜å‚¨å’Œæ£€ç´¢æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.cleanup_functions = []
        self.setup_signal_handlers()
        self.test_results = []
        self.api_calls = 0
        
    def setup_signal_handlers(self):
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
        def signal_handler(signum, frame):
            logger.info(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹æ¸…ç†...")
            self.cleanup_all()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        atexit.register(self.cleanup_all)
    
    def register_cleanup(self, func):
        """æ³¨å†Œæ¸…ç†å‡½æ•°"""
        self.cleanup_functions.append(func)
    
    def cleanup_all(self):
        """æ‰§è¡Œæ‰€æœ‰æ¸…ç†"""
        logger.info("ğŸ§¹ å¼€å§‹èµ„æºæ¸…ç†...")
        for func in self.cleanup_functions:
            try:
                if asyncio.iscoroutinefunction(func):
                    pass
                else:
                    func()
            except Exception as e:
                logger.debug(f"æ¸…ç†å‡½æ•°å¤±è´¥: {e}")
        logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
    
    async def test_memory_vector_store(self):
        """æµ‹è¯•å†…å­˜å‘é‡å­˜å‚¨"""
        logger.info("ğŸ§  æµ‹è¯•å†…å­˜å‘é‡å­˜å‚¨...")
        
        try:
            from document_stores import VectorStoreFactory
            
            # åˆ›å»ºå†…å­˜å‘é‡å­˜å‚¨
            config = {
                'type': 'memory',
                'vector_dimension': 1536,
                'similarity': 'dot_product'
            }
            
            vector_store = VectorStoreFactory.create(config)
            self.register_cleanup(lambda: vector_store.cleanup() if hasattr(vector_store, 'cleanup') else None)
            
            # ç”Ÿæˆæµ‹è¯•å‘é‡
            test_vectors = []
            test_documents = [
                "Vector databases are specialized for storing and querying high-dimensional vectors",
                "Machine learning models generate embeddings that represent semantic meaning",
                "Similarity search finds the most relevant documents based on vector distance",
                "FAISS is a library for efficient similarity search and clustering"
            ]
            
            # ä½¿ç”¨OpenAIç”ŸæˆçœŸå®embeddings
            import openai
            client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            for i, doc in enumerate(test_documents):
                response = client.embeddings.create(
                    model="text-embedding-ada-002",
                    input=doc
                )
                
                embedding = response.data[0].embedding
                test_vectors.append({
                    'id': f'doc_{i}',
                    'content': doc,
                    'vector': embedding,
                    'metadata': {'source': 'test', 'index': i}
                })
                self.api_calls += 1
                
                await asyncio.sleep(0.5)  # æ§åˆ¶APIé¢‘ç‡
            
            # æµ‹è¯•å‘é‡å­˜å‚¨
            stored_count = 0
            for item in test_vectors:
                try:
                    vector_store.add_document(
                        document_id=item['id'],
                        vector=item['vector'],
                        metadata={'content': item['content'], **item['metadata']}
                    )
                    stored_count += 1
                except Exception as e:
                    logger.warning(f"å­˜å‚¨æ–‡æ¡£å¤±è´¥ {item['id']}: {e}")
            
            # æµ‹è¯•æ£€ç´¢
            query_response = client.embeddings.create(
                model="text-embedding-ada-002",
                input="What is vector similarity search?"
            )
            query_vector = query_response.data[0].embedding
            self.api_calls += 1
            
            # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
            search_results = vector_store.search(query_vector, top_k=3)
            
            self.test_results.append({
                'test': 'memory_vector_store',
                'status': 'PASSED',
                'details': {
                    'stored_documents': stored_count,
                    'total_documents': len(test_documents),
                    'search_results_count': len(search_results),
                    'top_result_score': search_results[0]['score'] if search_results else 0,
                    'api_calls': len(test_documents) + 1
                }
            })
            
            logger.info(f"âœ… å†…å­˜å‘é‡å­˜å‚¨æµ‹è¯•æˆåŠŸ - å­˜å‚¨: {stored_count}, æ£€ç´¢: {len(search_results)}")
            return vector_store, test_vectors
            
        except Exception as e:
            logger.error(f"âŒ å†…å­˜å‘é‡å­˜å‚¨æµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                'test': 'memory_vector_store',
                'status': 'FAILED',
                'error': str(e)
            })
            return None, []
    
    async def test_faiss_vector_store(self):
        """æµ‹è¯•FAISSå‘é‡å­˜å‚¨"""
        logger.info("âš¡ æµ‹è¯•FAISSå‘é‡å­˜å‚¨...")
        
        try:
            from document_stores import VectorStoreFactory
            
            # åˆ›å»ºFAISSå‘é‡å­˜å‚¨
            config = {
                'type': 'faiss',
                'vector_dimension': 1536,
                'similarity': 'cosine',
                'faiss': {
                    'index_type': 'Flat',
                    'storage_path': './data/test_faiss',
                    'save_index': True
                }
            }
            
            vector_store = VectorStoreFactory.create(config)
            self.register_cleanup(lambda: vector_store.cleanup() if hasattr(vector_store, 'cleanup') else None)
            
            # åˆ›å»ºæµ‹è¯•å‘é‡é›†åˆ
            import openai
            client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            test_docs = [
                "Neural networks process information through interconnected layers",
                "Deep learning models can extract complex patterns from data",
                "Attention mechanisms help models focus on relevant information",
                "Transformer architecture revolutionized natural language processing"
            ]
            
            stored_docs = []
            for i, doc in enumerate(test_docs):
                response = client.embeddings.create(
                    model="text-embedding-ada-002",
                    input=doc
                )
                
                embedding = response.data[0].embedding
                doc_id = f'faiss_doc_{i}'
                
                # å­˜å‚¨åˆ°FAISS
                vector_store.add_document(
                    document_id=doc_id,
                    vector=embedding,
                    metadata={'content': doc, 'type': 'neural_networks'}
                )
                
                stored_docs.append({'id': doc_id, 'content': doc})
                self.api_calls += 1
                await asyncio.sleep(0.5)
            
            # æµ‹è¯•æŸ¥è¯¢
            query = "How do neural networks learn patterns?"
            query_response = client.embeddings.create(
                model="text-embedding-ada-002",
                input=query
            )
            query_vector = query_response.data[0].embedding
            self.api_calls += 1
            
            # FAISSæœç´¢
            search_results = vector_store.search(query_vector, top_k=2)
            
            # æµ‹è¯•æŒä¹…åŒ–å’ŒåŠ è½½
            vector_store.save_index()
            document_count = vector_store.get_document_count()
            
            self.test_results.append({
                'test': 'faiss_vector_store',
                'status': 'PASSED',
                'details': {
                    'stored_documents': len(stored_docs),
                    'document_count': document_count,
                    'search_results': len(search_results),
                    'top_similarity': search_results[0]['score'] if search_results else 0,
                    'persistence_test': 'passed',
                    'api_calls': len(test_docs) + 1
                }
            })
            
            logger.info(f"âœ… FAISSå‘é‡å­˜å‚¨æµ‹è¯•æˆåŠŸ - æ–‡æ¡£: {document_count}, æ£€ç´¢: {len(search_results)}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ FAISSå‘é‡å­˜å‚¨æµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                'test': 'faiss_vector_store',
                'status': 'FAILED', 
                'error': str(e)
            })
            return False
    
    async def test_semantic_search_quality(self):
        """æµ‹è¯•è¯­ä¹‰æœç´¢è´¨é‡"""
        logger.info("ğŸ¯ æµ‹è¯•è¯­ä¹‰æœç´¢è´¨é‡...")
        
        try:
            import openai
            client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            # æµ‹è¯•è¯­ä¹‰ç›¸å…³æ€§
            test_cases = [
                {
                    'documents': [
                        "Python is a programming language",
                        "Machine learning algorithms",
                        "Data science workflows",
                        "Statistical analysis methods"
                    ],
                    'query': "programming languages and coding",
                    'expected_top': 0  # æœŸæœ›ç¬¬ä¸€ä¸ªæ–‡æ¡£æ’åæœ€é«˜
                },
                {
                    'documents': [
                        "Database management systems",
                        "Machine learning models",
                        "Deep neural networks",
                        "Computer vision applications"
                    ],
                    'query': "artificial intelligence and ML",
                    'expected_top': 1  # æœŸæœ›ç¬¬äºŒä¸ªæ–‡æ¡£æ’åæœ€é«˜
                }
            ]
            
            quality_scores = []
            
            for case_idx, test_case in enumerate(test_cases):
                # ç”Ÿæˆæ–‡æ¡£embeddings
                doc_embeddings = []
                for doc in test_case['documents']:
                    response = client.embeddings.create(
                        model="text-embedding-ada-002",
                        input=doc
                    )
                    doc_embeddings.append(response.data[0].embedding)
                    self.api_calls += 1
                    await asyncio.sleep(0.3)
                
                # ç”ŸæˆæŸ¥è¯¢embedding
                query_response = client.embeddings.create(
                    model="text-embedding-ada-002",
                    input=test_case['query']
                )
                query_embedding = np.array(query_response.data[0].embedding)
                self.api_calls += 1
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarities = []
                for i, doc_emb in enumerate(doc_embeddings):
                    doc_vector = np.array(doc_emb)
                    # ä½™å¼¦ç›¸ä¼¼åº¦
                    similarity = np.dot(query_embedding, doc_vector) / (
                        np.linalg.norm(query_embedding) * np.linalg.norm(doc_vector)
                    )
                    similarities.append({'index': i, 'similarity': float(similarity)})
                
                # æ’åºå¹¶æ£€æŸ¥è´¨é‡
                similarities.sort(key=lambda x: x['similarity'], reverse=True)
                top_result_index = similarities[0]['index']
                
                quality_score = 1.0 if top_result_index == test_case['expected_top'] else 0.5
                quality_scores.append({
                    'case': case_idx,
                    'query': test_case['query'],
                    'expected_top': test_case['expected_top'],
                    'actual_top': top_result_index,
                    'top_similarity': similarities[0]['similarity'],
                    'quality_score': quality_score
                })
            
            avg_quality = sum(case['quality_score'] for case in quality_scores) / len(quality_scores)
            
            self.test_results.append({
                'test': 'semantic_search_quality',
                'status': 'PASSED',
                'details': {
                    'test_cases': len(test_cases),
                    'quality_scores': quality_scores,
                    'average_quality': avg_quality,
                    'api_calls': sum(len(case['documents']) + 1 for case in test_cases)
                }
            })
            
            logger.info(f"âœ… è¯­ä¹‰æœç´¢è´¨é‡æµ‹è¯•å®Œæˆ - å¹³å‡è´¨é‡åˆ†: {avg_quality:.2f}")
            return avg_quality > 0.7
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰æœç´¢è´¨é‡æµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                'test': 'semantic_search_quality',
                'status': 'FAILED',
                'error': str(e)
            })
            return False
    
    async def test_vector_storage_performance(self):
        """æµ‹è¯•å‘é‡å­˜å‚¨æ€§èƒ½"""
        logger.info("ğŸ“Š æµ‹è¯•å‘é‡å­˜å‚¨æ€§èƒ½...")
        
        try:
            from document_stores import VectorStoreFactory
            import time
            
            # åˆ›å»ºæ€§èƒ½æµ‹è¯•å‘é‡å­˜å‚¨
            config = {
                'type': 'memory',
                'vector_dimension': 1536,
                'similarity': 'dot_product'
            }
            
            vector_store = VectorStoreFactory.create(config)
            self.register_cleanup(lambda: vector_store.cleanup() if hasattr(vector_store, 'cleanup') else None)
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_size = 50  # æ§åˆ¶æµ‹è¯•è§„æ¨¡
            test_vectors = []
            
            import openai
            client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            # æ‰¹é‡ç”Ÿæˆembeddingï¼ˆæ§åˆ¶æˆæœ¬ï¼‰
            batch_texts = [f"Performance test document {i} with unique content about topic {i%5}" 
                          for i in range(min(test_size, 10))]  # é™åˆ¶APIè°ƒç”¨
            
            for i, text in enumerate(batch_texts):
                response = client.embeddings.create(
                    model="text-embedding-ada-002",
                    input=text
                )
                
                # å¤åˆ¶embeddingåˆ›å»ºæ›´å¤šæµ‹è¯•æ•°æ®
                base_embedding = np.array(response.data[0].embedding)
                for j in range(5):  # æ¯ä¸ªembeddingåˆ›å»º5ä¸ªå˜ç§
                    # æ·»åŠ å°çš„éšæœºå™ªå£°åˆ›å»ºå˜ç§
                    noise = np.random.normal(0, 0.01, base_embedding.shape)
                    variant_embedding = base_embedding + noise
                    
                    test_vectors.append({
                        'id': f'perf_doc_{i}_{j}',
                        'vector': variant_embedding.tolist(),
                        'content': f"{text} variant {j}"
                    })
                
                self.api_calls += 1
                await asyncio.sleep(0.5)
            
            # æ€§èƒ½æµ‹è¯•ï¼šæ‰¹é‡æ’å…¥
            start_time = time.time()
            inserted_count = 0
            
            for item in test_vectors:
                vector_store.add_document(
                    document_id=item['id'],
                    vector=item['vector'],
                    metadata={'content': item['content']}
                )
                inserted_count += 1
            
            insert_time = time.time() - start_time
            
            # æ€§èƒ½æµ‹è¯•ï¼šæœç´¢
            query_vector = test_vectors[0]['vector']  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå‘é‡ä½œä¸ºæŸ¥è¯¢
            
            search_times = []
            for _ in range(5):  # æ‰§è¡Œ5æ¬¡æœç´¢æµ‹è¯•
                start_time = time.time()
                results = vector_store.search(query_vector, top_k=10)
                search_time = time.time() - start_time
                search_times.append(search_time)
            
            avg_search_time = sum(search_times) / len(search_times)
            
            performance_metrics = {
                'total_documents': inserted_count,
                'insert_time_seconds': round(insert_time, 3),
                'insert_rate_docs_per_sec': round(inserted_count / insert_time, 2),
                'average_search_time_ms': round(avg_search_time * 1000, 2),
                'search_results_per_query': len(results) if 'results' in locals() else 0
            }
            
            self.test_results.append({
                'test': 'vector_storage_performance',
                'status': 'PASSED',
                'details': performance_metrics
            })
            
            logger.info(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ - æ’å…¥: {performance_metrics['insert_rate_docs_per_sec']} docs/sec, æœç´¢: {performance_metrics['average_search_time_ms']} ms")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡å­˜å‚¨æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                'test': 'vector_storage_performance',
                'status': 'FAILED',
                'error': str(e)
            })
            return False
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰å‘é‡å­˜å‚¨å’Œæ£€ç´¢æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹å‘é‡å­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½æµ‹è¯•...")
        start_time = datetime.now()
        
        tests = [
            ("å†…å­˜å‘é‡å­˜å‚¨", self.test_memory_vector_store),
            ("FAISSå‘é‡å­˜å‚¨", self.test_faiss_vector_store),
            ("è¯­ä¹‰æœç´¢è´¨é‡", self.test_semantic_search_quality),
            ("å‘é‡å­˜å‚¨æ€§èƒ½", self.test_vector_storage_performance)
        ]
        
        passed = 0
        failed = 0
        
        for test_name, test_func in tests:
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ§ª æ‰§è¡Œæµ‹è¯•: {test_name}")
            logger.info(f"{'='*60}")
            
            try:
                if test_name == "å†…å­˜å‘é‡å­˜å‚¨":
                    result, _ = await test_func()
                    result = result is not None
                else:
                    result = await test_func()
                
                if result:
                    passed += 1
                    logger.info(f"âœ… {test_name} - é€šè¿‡")
                else:
                    failed += 1
                    logger.error(f"âŒ {test_name} - å¤±è´¥")
                
                await asyncio.sleep(1)
                
            except Exception as e:
                failed += 1
                logger.error(f"âŒ {test_name} - å¼‚å¸¸: {e}")
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        await self.generate_test_report(passed, failed, start_time)
        
        return failed == 0
    
    async def generate_test_report(self, passed: int, failed: int, start_time: datetime):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        end_time = datetime.now()
        duration = end_time - start_time
        
        report = {
            'test_session': {
                'test_type': 'Vector Storage and Retrieval Test',
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': duration.total_seconds(),
                'total_tests': len(self.test_results),
                'passed': passed,
                'failed': failed,
                'success_rate': f"{(passed / len(self.test_results) * 100):.1f}%" if self.test_results else "0%"
            },
            'api_usage': {
                'total_api_calls': self.api_calls,
                'estimated_cost': round(self.api_calls * 0.0001, 6)
            },
            'test_results': self.test_results
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_dir = Path("test_reports")
        report_dir.mkdir(exist_ok=True)
        
        report_path = report_dir / f"vector_storage_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # æ‰“å°æ‘˜è¦
        self._print_summary(report)
    
    def _print_summary(self, report):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print(f"\n{'='*80}")
        print("ğŸ§ª å‘é‡å­˜å‚¨å’Œæ£€ç´¢åŠŸèƒ½æµ‹è¯•æŠ¥å‘Š")
        print(f"{'='*80}")
        print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {datetime.fromisoformat(report['test_session']['start_time']).strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸ æµ‹è¯•æ—¶é•¿: {report['test_session']['duration_seconds']:.2f} ç§’")
        print(f"ğŸ“Š æ€»æµ‹è¯•æ•°: {report['test_session']['total_tests']}")
        print(f"âœ… é€šè¿‡: {report['test_session']['passed']}")
        print(f"âŒ å¤±è´¥: {report['test_session']['failed']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {report['test_session']['success_rate']}")
        print(f"ğŸ”‘ APIè°ƒç”¨: {report['api_usage']['total_api_calls']} æ¬¡")
        print(f"ğŸ’° ä¼°ç®—æˆæœ¬: ${report['api_usage']['estimated_cost']:.6f}")
        print("")
        
        print("ğŸ“‹ æµ‹è¯•è¯¦æƒ…:")
        for result in self.test_results:
            status_icon = "âœ…" if result['status'] == 'PASSED' else "âŒ"
            print(f"  {status_icon} {result['test']}")
            if result['status'] == 'FAILED' and 'error' in result:
                print(f"      é”™è¯¯: {result['error']}")
        
        print(f"{'='*80}")

async def main():
    """ä¸»å‡½æ•°"""
    tester = VectorStorageRetrievalTester()
    
    try:
        success = await tester.run_all_tests()
        if success:
            print("ğŸ‰ æ‰€æœ‰å‘é‡å­˜å‚¨å’Œæ£€ç´¢æµ‹è¯•é€šè¿‡ï¼")
            return 0
        else:
            print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æŠ¥å‘Š")
            return 1
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        return 1
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¼‚å¸¸: {e}")
        return 1
    finally:
        tester.cleanup_all()

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)