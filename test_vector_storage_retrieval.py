#!/usr/bin/env python3
"""
MCP Academic RAG Server - 向量存储和检索功能测试
验证FAISS、Memory向量存储和语义检索功能
"""

import sys
import os
import asyncio
import logging
import signal
import atexit
import json
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# 添加项目根目录
sys.path.insert(0, os.path.abspath('.'))

# 设置日志
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VectorStorageRetrievalTester:
    """向量存储和检索测试器"""
    
    def __init__(self):
        self.cleanup_functions = []
        self.setup_signal_handlers()
        self.test_results = []
        self.api_calls = 0
        
    def setup_signal_handlers(self):
        """设置信号处理器"""
        def signal_handler(signum, frame):
            logger.info(f"收到信号 {signum}，开始清理...")
            self.cleanup_all()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        atexit.register(self.cleanup_all)
    
    def register_cleanup(self, func):
        """注册清理函数"""
        self.cleanup_functions.append(func)
    
    def cleanup_all(self):
        """执行所有清理"""
        logger.info("🧹 开始资源清理...")
        for func in self.cleanup_functions:
            try:
                if asyncio.iscoroutinefunction(func):
                    pass
                else:
                    func()
            except Exception as e:
                logger.debug(f"清理函数失败: {e}")
        logger.info("✅ 资源清理完成")
    
    async def test_memory_vector_store(self):
        """测试内存向量存储"""
        logger.info("🧠 测试内存向量存储...")
        
        try:
            from document_stores import VectorStoreFactory
            
            # 创建内存向量存储
            config = {
                'type': 'memory',
                'vector_dimension': 1536,
                'similarity': 'dot_product'
            }
            
            vector_store = VectorStoreFactory.create(config)
            self.register_cleanup(lambda: vector_store.cleanup() if hasattr(vector_store, 'cleanup') else None)
            
            # 生成测试向量
            test_vectors = []
            test_documents = [
                "Vector databases are specialized for storing and querying high-dimensional vectors",
                "Machine learning models generate embeddings that represent semantic meaning",
                "Similarity search finds the most relevant documents based on vector distance",
                "FAISS is a library for efficient similarity search and clustering"
            ]
            
            # 使用OpenAI生成真实embeddings
            import openai
            client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            for i, doc in enumerate(test_documents):
                response = client.embeddings.create(
                    model="text-embedding-ada-002",
                    input=doc
                )
                
                embedding = response.data[0].embedding
                test_vectors.append({
                    'id': f'doc_{i}',
                    'content': doc,
                    'vector': embedding,
                    'metadata': {'source': 'test', 'index': i}
                })
                self.api_calls += 1
                
                await asyncio.sleep(0.5)  # 控制API频率
            
            # 测试向量存储
            stored_count = 0
            for item in test_vectors:
                try:
                    vector_store.add_document(
                        document_id=item['id'],
                        vector=item['vector'],
                        metadata={'content': item['content'], **item['metadata']}
                    )
                    stored_count += 1
                except Exception as e:
                    logger.warning(f"存储文档失败 {item['id']}: {e}")
            
            # 测试检索
            query_response = client.embeddings.create(
                model="text-embedding-ada-002",
                input="What is vector similarity search?"
            )
            query_vector = query_response.data[0].embedding
            self.api_calls += 1
            
            # 执行相似度搜索
            search_results = vector_store.search(query_vector, top_k=3)
            
            self.test_results.append({
                'test': 'memory_vector_store',
                'status': 'PASSED',
                'details': {
                    'stored_documents': stored_count,
                    'total_documents': len(test_documents),
                    'search_results_count': len(search_results),
                    'top_result_score': search_results[0]['score'] if search_results else 0,
                    'api_calls': len(test_documents) + 1
                }
            })
            
            logger.info(f"✅ 内存向量存储测试成功 - 存储: {stored_count}, 检索: {len(search_results)}")
            return vector_store, test_vectors
            
        except Exception as e:
            logger.error(f"❌ 内存向量存储测试失败: {e}")
            self.test_results.append({
                'test': 'memory_vector_store',
                'status': 'FAILED',
                'error': str(e)
            })
            return None, []
    
    async def test_faiss_vector_store(self):
        """测试FAISS向量存储"""
        logger.info("⚡ 测试FAISS向量存储...")
        
        try:
            from document_stores import VectorStoreFactory
            
            # 创建FAISS向量存储
            config = {
                'type': 'faiss',
                'vector_dimension': 1536,
                'similarity': 'cosine',
                'faiss': {
                    'index_type': 'Flat',
                    'storage_path': './data/test_faiss',
                    'save_index': True
                }
            }
            
            vector_store = VectorStoreFactory.create(config)
            self.register_cleanup(lambda: vector_store.cleanup() if hasattr(vector_store, 'cleanup') else None)
            
            # 创建测试向量集合
            import openai
            client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            test_docs = [
                "Neural networks process information through interconnected layers",
                "Deep learning models can extract complex patterns from data",
                "Attention mechanisms help models focus on relevant information",
                "Transformer architecture revolutionized natural language processing"
            ]
            
            stored_docs = []
            for i, doc in enumerate(test_docs):
                response = client.embeddings.create(
                    model="text-embedding-ada-002",
                    input=doc
                )
                
                embedding = response.data[0].embedding
                doc_id = f'faiss_doc_{i}'
                
                # 存储到FAISS
                vector_store.add_document(
                    document_id=doc_id,
                    vector=embedding,
                    metadata={'content': doc, 'type': 'neural_networks'}
                )
                
                stored_docs.append({'id': doc_id, 'content': doc})
                self.api_calls += 1
                await asyncio.sleep(0.5)
            
            # 测试查询
            query = "How do neural networks learn patterns?"
            query_response = client.embeddings.create(
                model="text-embedding-ada-002",
                input=query
            )
            query_vector = query_response.data[0].embedding
            self.api_calls += 1
            
            # FAISS搜索
            search_results = vector_store.search(query_vector, top_k=2)
            
            # 测试持久化和加载
            vector_store.save_index()
            document_count = vector_store.get_document_count()
            
            self.test_results.append({
                'test': 'faiss_vector_store',
                'status': 'PASSED',
                'details': {
                    'stored_documents': len(stored_docs),
                    'document_count': document_count,
                    'search_results': len(search_results),
                    'top_similarity': search_results[0]['score'] if search_results else 0,
                    'persistence_test': 'passed',
                    'api_calls': len(test_docs) + 1
                }
            })
            
            logger.info(f"✅ FAISS向量存储测试成功 - 文档: {document_count}, 检索: {len(search_results)}")
            return True
            
        except Exception as e:
            logger.error(f"❌ FAISS向量存储测试失败: {e}")
            self.test_results.append({
                'test': 'faiss_vector_store',
                'status': 'FAILED', 
                'error': str(e)
            })
            return False
    
    async def test_semantic_search_quality(self):
        """测试语义搜索质量"""
        logger.info("🎯 测试语义搜索质量...")
        
        try:
            import openai
            client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            # 测试语义相关性
            test_cases = [
                {
                    'documents': [
                        "Python is a programming language",
                        "Machine learning algorithms",
                        "Data science workflows",
                        "Statistical analysis methods"
                    ],
                    'query': "programming languages and coding",
                    'expected_top': 0  # 期望第一个文档排名最高
                },
                {
                    'documents': [
                        "Database management systems",
                        "Machine learning models",
                        "Deep neural networks",
                        "Computer vision applications"
                    ],
                    'query': "artificial intelligence and ML",
                    'expected_top': 1  # 期望第二个文档排名最高
                }
            ]
            
            quality_scores = []
            
            for case_idx, test_case in enumerate(test_cases):
                # 生成文档embeddings
                doc_embeddings = []
                for doc in test_case['documents']:
                    response = client.embeddings.create(
                        model="text-embedding-ada-002",
                        input=doc
                    )
                    doc_embeddings.append(response.data[0].embedding)
                    self.api_calls += 1
                    await asyncio.sleep(0.3)
                
                # 生成查询embedding
                query_response = client.embeddings.create(
                    model="text-embedding-ada-002",
                    input=test_case['query']
                )
                query_embedding = np.array(query_response.data[0].embedding)
                self.api_calls += 1
                
                # 计算相似度
                similarities = []
                for i, doc_emb in enumerate(doc_embeddings):
                    doc_vector = np.array(doc_emb)
                    # 余弦相似度
                    similarity = np.dot(query_embedding, doc_vector) / (
                        np.linalg.norm(query_embedding) * np.linalg.norm(doc_vector)
                    )
                    similarities.append({'index': i, 'similarity': float(similarity)})
                
                # 排序并检查质量
                similarities.sort(key=lambda x: x['similarity'], reverse=True)
                top_result_index = similarities[0]['index']
                
                quality_score = 1.0 if top_result_index == test_case['expected_top'] else 0.5
                quality_scores.append({
                    'case': case_idx,
                    'query': test_case['query'],
                    'expected_top': test_case['expected_top'],
                    'actual_top': top_result_index,
                    'top_similarity': similarities[0]['similarity'],
                    'quality_score': quality_score
                })
            
            avg_quality = sum(case['quality_score'] for case in quality_scores) / len(quality_scores)
            
            self.test_results.append({
                'test': 'semantic_search_quality',
                'status': 'PASSED',
                'details': {
                    'test_cases': len(test_cases),
                    'quality_scores': quality_scores,
                    'average_quality': avg_quality,
                    'api_calls': sum(len(case['documents']) + 1 for case in test_cases)
                }
            })
            
            logger.info(f"✅ 语义搜索质量测试完成 - 平均质量分: {avg_quality:.2f}")
            return avg_quality > 0.7
            
        except Exception as e:
            logger.error(f"❌ 语义搜索质量测试失败: {e}")
            self.test_results.append({
                'test': 'semantic_search_quality',
                'status': 'FAILED',
                'error': str(e)
            })
            return False
    
    async def test_vector_storage_performance(self):
        """测试向量存储性能"""
        logger.info("📊 测试向量存储性能...")
        
        try:
            from document_stores import VectorStoreFactory
            import time
            
            # 创建性能测试向量存储
            config = {
                'type': 'memory',
                'vector_dimension': 1536,
                'similarity': 'dot_product'
            }
            
            vector_store = VectorStoreFactory.create(config)
            self.register_cleanup(lambda: vector_store.cleanup() if hasattr(vector_store, 'cleanup') else None)
            
            # 生成测试数据
            test_size = 50  # 控制测试规模
            test_vectors = []
            
            import openai
            client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            # 批量生成embedding（控制成本）
            batch_texts = [f"Performance test document {i} with unique content about topic {i%5}" 
                          for i in range(min(test_size, 10))]  # 限制API调用
            
            for i, text in enumerate(batch_texts):
                response = client.embeddings.create(
                    model="text-embedding-ada-002",
                    input=text
                )
                
                # 复制embedding创建更多测试数据
                base_embedding = np.array(response.data[0].embedding)
                for j in range(5):  # 每个embedding创建5个变种
                    # 添加小的随机噪声创建变种
                    noise = np.random.normal(0, 0.01, base_embedding.shape)
                    variant_embedding = base_embedding + noise
                    
                    test_vectors.append({
                        'id': f'perf_doc_{i}_{j}',
                        'vector': variant_embedding.tolist(),
                        'content': f"{text} variant {j}"
                    })
                
                self.api_calls += 1
                await asyncio.sleep(0.5)
            
            # 性能测试：批量插入
            start_time = time.time()
            inserted_count = 0
            
            for item in test_vectors:
                vector_store.add_document(
                    document_id=item['id'],
                    vector=item['vector'],
                    metadata={'content': item['content']}
                )
                inserted_count += 1
            
            insert_time = time.time() - start_time
            
            # 性能测试：搜索
            query_vector = test_vectors[0]['vector']  # 使用第一个向量作为查询
            
            search_times = []
            for _ in range(5):  # 执行5次搜索测试
                start_time = time.time()
                results = vector_store.search(query_vector, top_k=10)
                search_time = time.time() - start_time
                search_times.append(search_time)
            
            avg_search_time = sum(search_times) / len(search_times)
            
            performance_metrics = {
                'total_documents': inserted_count,
                'insert_time_seconds': round(insert_time, 3),
                'insert_rate_docs_per_sec': round(inserted_count / insert_time, 2),
                'average_search_time_ms': round(avg_search_time * 1000, 2),
                'search_results_per_query': len(results) if 'results' in locals() else 0
            }
            
            self.test_results.append({
                'test': 'vector_storage_performance',
                'status': 'PASSED',
                'details': performance_metrics
            })
            
            logger.info(f"✅ 性能测试完成 - 插入: {performance_metrics['insert_rate_docs_per_sec']} docs/sec, 搜索: {performance_metrics['average_search_time_ms']} ms")
            return True
            
        except Exception as e:
            logger.error(f"❌ 向量存储性能测试失败: {e}")
            self.test_results.append({
                'test': 'vector_storage_performance',
                'status': 'FAILED',
                'error': str(e)
            })
            return False
    
    async def run_all_tests(self):
        """运行所有向量存储和检索测试"""
        logger.info("🚀 开始向量存储和检索功能测试...")
        start_time = datetime.now()
        
        tests = [
            ("内存向量存储", self.test_memory_vector_store),
            ("FAISS向量存储", self.test_faiss_vector_store),
            ("语义搜索质量", self.test_semantic_search_quality),
            ("向量存储性能", self.test_vector_storage_performance)
        ]
        
        passed = 0
        failed = 0
        
        for test_name, test_func in tests:
            logger.info(f"\n{'='*60}")
            logger.info(f"🧪 执行测试: {test_name}")
            logger.info(f"{'='*60}")
            
            try:
                if test_name == "内存向量存储":
                    result, _ = await test_func()
                    result = result is not None
                else:
                    result = await test_func()
                
                if result:
                    passed += 1
                    logger.info(f"✅ {test_name} - 通过")
                else:
                    failed += 1
                    logger.error(f"❌ {test_name} - 失败")
                
                await asyncio.sleep(1)
                
            except Exception as e:
                failed += 1
                logger.error(f"❌ {test_name} - 异常: {e}")
        
        # 生成测试报告
        await self.generate_test_report(passed, failed, start_time)
        
        return failed == 0
    
    async def generate_test_report(self, passed: int, failed: int, start_time: datetime):
        """生成测试报告"""
        end_time = datetime.now()
        duration = end_time - start_time
        
        report = {
            'test_session': {
                'test_type': 'Vector Storage and Retrieval Test',
                'start_time': start_time.isoformat(),
                'end_time': end_time.isoformat(),
                'duration_seconds': duration.total_seconds(),
                'total_tests': len(self.test_results),
                'passed': passed,
                'failed': failed,
                'success_rate': f"{(passed / len(self.test_results) * 100):.1f}%" if self.test_results else "0%"
            },
            'api_usage': {
                'total_api_calls': self.api_calls,
                'estimated_cost': round(self.api_calls * 0.0001, 6)
            },
            'test_results': self.test_results
        }
        
        # 保存报告
        report_dir = Path("test_reports")
        report_dir.mkdir(exist_ok=True)
        
        report_path = report_dir / f"vector_storage_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"📊 测试报告已保存: {report_path}")
        
        # 打印摘要
        self._print_summary(report)
    
    def _print_summary(self, report):
        """打印测试摘要"""
        print(f"\n{'='*80}")
        print("🧪 向量存储和检索功能测试报告")
        print(f"{'='*80}")
        print(f"📅 测试时间: {datetime.fromisoformat(report['test_session']['start_time']).strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"⏱️ 测试时长: {report['test_session']['duration_seconds']:.2f} 秒")
        print(f"📊 总测试数: {report['test_session']['total_tests']}")
        print(f"✅ 通过: {report['test_session']['passed']}")
        print(f"❌ 失败: {report['test_session']['failed']}")
        print(f"📈 成功率: {report['test_session']['success_rate']}")
        print(f"🔑 API调用: {report['api_usage']['total_api_calls']} 次")
        print(f"💰 估算成本: ${report['api_usage']['estimated_cost']:.6f}")
        print("")
        
        print("📋 测试详情:")
        for result in self.test_results:
            status_icon = "✅" if result['status'] == 'PASSED' else "❌"
            print(f"  {status_icon} {result['test']}")
            if result['status'] == 'FAILED' and 'error' in result:
                print(f"      错误: {result['error']}")
        
        print(f"{'='*80}")

async def main():
    """主函数"""
    tester = VectorStorageRetrievalTester()
    
    try:
        success = await tester.run_all_tests()
        if success:
            print("🎉 所有向量存储和检索测试通过！")
            return 0
        else:
            print("⚠️ 部分测试失败，请检查报告")
            return 1
    except KeyboardInterrupt:
        print("\n⚠️ 用户中断测试")
        return 1
    except Exception as e:
        print(f"\n❌ 测试异常: {e}")
        return 1
    finally:
        tester.cleanup_all()

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)